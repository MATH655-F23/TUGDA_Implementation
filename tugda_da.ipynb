{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%notebook logs/tugda_da.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from itertools import cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.autograd import Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = 'cpu' if torch.backends.mps.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReverseLayerF(Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, alpha):\n",
    "        ctx.alpha = alpha\n",
    "\n",
    "        return x.view_as(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        output = grad_output.neg() * ctx.alpha\n",
    "\n",
    "        return output, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (2652268761.py, line 192)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[96], line 192\u001b[0;36m\u001b[0m\n\u001b[0;31m    def test(self):\u001b[0m\n\u001b[0m                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "class Tugda_da(nn.Module):\n",
    "\n",
    "    def __init__(self,params,train_data,test_data,y_train,y_test):\n",
    "        super(Tugda_da,self).__init__()\n",
    "        self.learning_rate = params['lr']\n",
    "        self.batch_size = params['bs_source']\n",
    "        self.mu = params['mu']\n",
    "        self.lambda_ = params['lambda_']\n",
    "        self.gamma = params['gamma']\n",
    "        self.lambda_disc = params['lambda_disc']\n",
    "        self.bs_disc = params['bs_disc']\n",
    "        self.n_epoch = params['n_epochs']\n",
    "        self.passes = params['passes']\n",
    "        self.num_tasks = params['num_tasks']\n",
    "\n",
    "        self.train_data = train_data\n",
    "        self.y_train = y_train\n",
    "        self.test_data = test_data\n",
    "        self.y_test = y_test\n",
    "\n",
    "        input_dim=self.train_data.shape[1]\n",
    "\n",
    "        feature_extractor = [nn.Linear(input_dim, params['hidden_units_1']), \n",
    "                             nn.Dropout(p=params['dropout']),\n",
    "                             nn.ReLU()]\n",
    "        \n",
    "        self.feature_extractor = nn.Sequential(*feature_extractor) #L layer in diag\n",
    "\n",
    "        latent_basis =  [nn.Linear(params['hidden_units_1'], params['latent_space']),\n",
    "                         nn.Dropout(p=params['dropout']),\n",
    "                         nn.ReLU()]\n",
    "        \n",
    "        self.latent_basis = nn.Sequential(*latent_basis) #Z layer in diag\n",
    "        \n",
    "        #task-specific weights\n",
    "        self.MTL = nn.Linear(params['latent_space'], self.num_tasks) #S Layer in diag| also the prediction outputs\n",
    "        \n",
    "        #decoder weights\n",
    "        decTTF = [nn.Linear( self.num_tasks , params['latent_space']), nn.ReLU()] # A layer in diagram or decoder task to feature layer \n",
    "        self.decTTF = nn.Sequential(*decTTF) \n",
    "\n",
    "        #uncertainty (aleatoric)\n",
    "        self.log_vars = torch.zeros(self.num_tasks, requires_grad=True, device=device)\n",
    "        \n",
    "        #discriminator\n",
    "        task_classifier = [nn.Linear(params[\"latent_space\"], params[\"n_units_disc\"]), \n",
    "                             nn.Dropout(p=params['dropout']), \n",
    "                             nn.ReLU(),\n",
    "                             nn.Linear(params[\"n_units_disc\"], 1), \n",
    "                             nn.Sigmoid()\n",
    "                            ]\n",
    "        self.task_classifier = nn.Sequential(*task_classifier)\n",
    "\n",
    "        self.prepare_data()\n",
    "\n",
    "    def forward(self, input_data, alpha):\n",
    "        x = self.feature_extractor(input_data) #usually the input data is treated as x\n",
    "        x = self.latent_basis(x) # here h \n",
    "        preds = self.MTL(x)\n",
    "        x_hat = self.decTTF(preds) # task transfer to features # here was h_hat\n",
    "        reverse_feature = ReverseLayerF.apply(x, alpha)\n",
    "        task_classifier_output = self.task_classifier(reverse_feature)\n",
    "        return preds, x, x_hat, task_classifier_output\n",
    "    \n",
    "    def prepare_data(self):\n",
    "        train_dataset = TensorDataset(torch.FloatTensor(self.train_data),\n",
    "                                      torch.FloatTensor(self.y_train))\n",
    "\n",
    "        \n",
    "        target_unl_dataset = TensorDataset(torch.FloatTensor(self.test_data))\n",
    "        \n",
    "        self.train_dataset = train_dataset\n",
    "        self.target_unl_dataset = target_unl_dataset\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        dataloader1 = DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True) #in vitro data\n",
    "        dataloader2 = DataLoader(self.target_unl_dataset, batch_size=self.bs_disc, shuffle=True) #in vivo data \n",
    "        self.len_dataloader = min(len(dataloader1), len(dataloader2))\n",
    "        x=zip(dataloader1, cycle(dataloader2))\n",
    "        return x\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        dataloader = DataLoader(self.target_unl_dataset, batch_size=len(self.target_unl_dataset), shuffle=False)\n",
    "        return dataloader\n",
    "    \n",
    "    def MSE_loss(self, x, x_hat):\n",
    "        mse_loss = torch.nn.MSELoss()\n",
    "        return mse_loss(x, x_hat)\n",
    "    \n",
    "    def task_mse_ignore_nan(self, preds, labels):\n",
    "        mse_loss = torch.nn.MSELoss(reduction='none')\n",
    "        per_task_loss = torch.zeros(labels.size(1), device=device)\n",
    "\n",
    "        for k in range(labels.size(1)):\n",
    "            precision = torch.exp(-self.log_vars[k]) #didnt understand this.\n",
    "            diff = mse_loss(preds[~torch.isnan(labels[:,k]), k], labels[~torch.isnan(labels[:,k]), k])\n",
    "            per_task_loss[k] = torch.mean(precision * diff + self.log_vars[k])\n",
    "\n",
    "        return torch.mean(per_task_loss[~torch.isnan(per_task_loss)]), per_task_loss\n",
    "\n",
    "    def mse_ignore_nan_test(self, preds, labels):\n",
    "        mse_loss = torch.nn.MSELoss(reduction='mean')\n",
    "        per_task_loss = torch.zeros(labels.size(1), device=device)\n",
    "        per_sample_loss = torch.zeros(labels.size(0), device=device)\n",
    "\n",
    "        \n",
    "        for k in range(labels.size(1)):\n",
    "            per_task_loss[k] = mse_loss(preds[~torch.isnan(labels[:,k]), k], labels[~torch.isnan(labels[:,k]), k])\n",
    "        \n",
    "        #per class loss\n",
    "        for k in range(labels.size(0)):\n",
    "            per_sample_loss[k] = mse_loss(preds[k, ~torch.isnan(labels[k,:])], labels[k, ~torch.isnan(labels[k, :])])\n",
    "        \n",
    "        \n",
    "        return torch.mean(per_task_loss[~torch.isnan(per_task_loss)]), per_task_loss,per_sample_loss\n",
    "    \n",
    "    def binary_classification_loss(self, preds, labels):\n",
    "        bin_loss = torch.nn.BCELoss()\n",
    "        return bin_loss(preds, labels)\n",
    "    \n",
    "    def train(self):\n",
    "\n",
    "        optimizer = torch.optim.Adagrad([\n",
    "            {'params': self.feature_extractor.parameters()},\n",
    "            {'params': self.latent_basis.parameters()},\n",
    "            {'params': self.MTL.parameters()},\n",
    "            {'params': self.decTTF.parameters()},\n",
    "            {'params': self.task_classifier.parameters()},\n",
    "            {'params': self.log_vars}\n",
    "        ], lr=self.learning_rate)\n",
    "        \n",
    "        for epoch in range(self.n_epoch):\n",
    "            train_dataloader=self.train_dataloader()\n",
    "            train_loss=0\n",
    "            for batch_idx,data in enumerate(train_dataloader):\n",
    "                x,y=data[0] #in vitro\n",
    "                x_unl=data[1][0] #invivo\n",
    "\n",
    "                p=float(batch_idx+epoch*self.len_dataloader)/self.n_epoch/self.len_dataloader\n",
    "                alpha=2. / (1. + np.exp(-10 * p)) - 1\n",
    "\n",
    "                preds_simulation=torch.zeros(y.size(0),y.size(1),self.passes)#,device=device) \n",
    "                for simulation in range(self.passes):\n",
    "                    preds, h, h_hat, domain_out_source = self.forward(x, alpha)\n",
    "                    preds_simulation[:,:, simulation]=preds\n",
    "                \n",
    "                preds_mean = torch.mean(preds_simulation, axis=2) #2D predictions after p passes\n",
    "                preds_var = torch.var(preds_simulation, axis=2) #2D\n",
    "                total_unc = torch.mean(preds_var, axis=0) # mu_t this is epistemic uncertainity 1D of tasks i.e y.shape[1]\n",
    "\n",
    "\n",
    "                m_loss, task_loss = self.task_mse_ignore_nan(preds_mean, y)\n",
    "                recon_loss= self.MSE_loss(h,h_hat)\n",
    "\n",
    "                a = 1 + (total_unc + torch.sum(torch.abs(self.decTTF[0].weight.T),1)) #multiplier to L_bnn\n",
    "\n",
    "                loss_weight = ( a[~torch.isnan(task_loss)] ) * task_loss[~torch.isnan(task_loss)]\n",
    "                loss_weight = torch.sum(loss_weight)\n",
    "\n",
    "                #Regularizer gamma(Z-sig(ZSA))\n",
    "                l1_S = self.mu * self.MTL.weight.norm(1)\n",
    "                L = self.latent_basis[0].weight.norm(2) + self.feature_extractor[0].weight.norm(2)\n",
    "                l2_L = self.lambda_ * L\n",
    "\n",
    "                #domain discriminator source\n",
    "                #normal data pass\n",
    "                # 0 is one class of invitro\n",
    "                zeros = torch.zeros(y.size(0),1)#, device=self.device)\n",
    "                d_loss_source = self.binary_classification_loss(domain_out_source, zeros)\n",
    "                \n",
    "                #domain discriminator target\n",
    "                #unlabelled data pass\n",
    "                #1 is second class which is invivo \n",
    "                preds, h, h_hat, domain_out_target = self.forward(x_unl, alpha)\n",
    "                ones = torch.ones(x_unl.size(0),1)#, device=self.device) \n",
    "                d_loss_target = self.binary_classification_loss(domain_out_target, ones)\n",
    "                \n",
    "                d_loss = d_loss_source + d_loss_target #l_adv loss or the adversarial loss \n",
    "\n",
    "                #total loss\n",
    "                loss = loss_weight + recon_loss + l1_S + l2_L + (self.lambda_disc *d_loss)\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                train_loss+=loss\n",
    "\n",
    "            train_loss/=self.len_dataloader\n",
    "            print('Epoch [{}/{}] : Loss {}'.format(epoch,self.n_epoch,train_loss))#, 'source_loss': task_loss, 'disc_loss': d_loss})\n",
    "        return None #loss, m_loss, d_loss\n",
    "    \n",
    "    def test(self):\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cell-line dataset;\n",
    "gdsc_dataset = pd.read_csv('data/GDSCDA_fpkm_AUC_all_drugs.zip', index_col=0)\n",
    "#gene set range\n",
    "gene_list = gdsc_dataset.columns[0:1780]\n",
    "drug_list = gdsc_dataset.columns[1780:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pdx novartis dataset;\n",
    "pdx_dataset = pd.read_csv('data/PDX_MTL_DA.csv', index_col=0)\n",
    "drugs_pdx = pdx_dataset.columns[1780:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_params = {\n",
    " 'hidden_units_1': 1500,\n",
    " 'latent_space': 800,\n",
    " 'lr': 0.001,\n",
    " 'dropout': 0.1,\n",
    " 'mu': 1,\n",
    " 'lambda_': 1,\n",
    " 'gamma': 0.01,\n",
    " 'n_units_disc': 500,\n",
    " 'n_epochs': 150, #50\n",
    " 'bs_disc': 64,\n",
    " 'bs_source': 300,\n",
    " 'lambda_disc': 0.3,\n",
    " 'num_tasks': 200,\n",
    " 'passes': 50}#50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_list = gdsc_dataset.columns[0:1780]\n",
    "drug_list = gdsc_dataset.columns[1780:]\n",
    "\n",
    "X_train = gdsc_dataset[gene_list].values #In vitro data\n",
    "y_train = gdsc_dataset[drug_list].values\n",
    " \n",
    "X_test = pdx_dataset[gene_list].values #In-Vivo data\n",
    "y_test = pdx_dataset[drugs_pdx].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/50] : Loss 5386.49462890625\n",
      "Epoch [1/50] : Loss 3371.791015625\n",
      "Epoch [2/50] : Loss 3117.201171875\n",
      "Epoch [3/50] : Loss 2766.5078125\n",
      "Epoch [4/50] : Loss 2597.6826171875\n",
      "Epoch [5/50] : Loss 2421.511962890625\n",
      "Epoch [6/50] : Loss 2270.666259765625\n",
      "Epoch [7/50] : Loss 2118.275390625\n",
      "Epoch [8/50] : Loss 1997.61328125\n",
      "Epoch [9/50] : Loss 1874.8709716796875\n",
      "Epoch [10/50] : Loss 1784.60107421875\n",
      "Epoch [11/50] : Loss 1682.4320068359375\n",
      "Epoch [12/50] : Loss 1584.6466064453125\n",
      "Epoch [13/50] : Loss 1505.9959716796875\n",
      "Epoch [14/50] : Loss 1433.216796875\n",
      "Epoch [15/50] : Loss 1369.0439453125\n",
      "Epoch [16/50] : Loss 1301.4708251953125\n",
      "Epoch [17/50] : Loss 1241.1123046875\n",
      "Epoch [18/50] : Loss 1190.5302734375\n",
      "Epoch [19/50] : Loss 1123.7413330078125\n",
      "Epoch [20/50] : Loss 1069.64013671875\n",
      "Epoch [21/50] : Loss 1030.6763916015625\n",
      "Epoch [22/50] : Loss 980.7717895507812\n",
      "Epoch [23/50] : Loss 938.091552734375\n",
      "Epoch [24/50] : Loss 907.1686401367188\n",
      "Epoch [25/50] : Loss 863.49658203125\n",
      "Epoch [26/50] : Loss 834.3906860351562\n",
      "Epoch [27/50] : Loss 804.7420043945312\n",
      "Epoch [28/50] : Loss 767.423828125\n",
      "Epoch [29/50] : Loss 746.9855346679688\n",
      "Epoch [30/50] : Loss 713.080078125\n",
      "Epoch [31/50] : Loss 690.7760620117188\n",
      "Epoch [32/50] : Loss 670.0885620117188\n",
      "Epoch [33/50] : Loss 646.6494750976562\n",
      "Epoch [34/50] : Loss 634.5482788085938\n",
      "Epoch [35/50] : Loss 611.1649780273438\n",
      "Epoch [36/50] : Loss 601.843994140625\n",
      "Epoch [37/50] : Loss 582.369140625\n",
      "Epoch [38/50] : Loss 572.3445434570312\n",
      "Epoch [39/50] : Loss 554.545654296875\n",
      "Epoch [40/50] : Loss 546.99365234375\n",
      "Epoch [41/50] : Loss 533.136474609375\n",
      "Epoch [42/50] : Loss 527.2135009765625\n",
      "Epoch [43/50] : Loss 517.4464721679688\n",
      "Epoch [44/50] : Loss 513.79296875\n",
      "Epoch [45/50] : Loss 506.0689697265625\n",
      "Epoch [46/50] : Loss 507.0711364746094\n",
      "Epoch [47/50] : Loss 499.1824035644531\n",
      "Epoch [48/50] : Loss 499.65234375\n",
      "Epoch [49/50] : Loss 494.1774597167969\n"
     ]
    }
   ],
   "source": [
    "model = Tugda_da(net_params, X_train, X_test, y_train, y_test)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
