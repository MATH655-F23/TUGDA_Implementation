{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "%notebook code_logs/tugda_da.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from itertools import cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.autograd import Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = 'cpu' if torch.backends.mps.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReverseLayerF(Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, alpha):\n",
    "        ctx.alpha = alpha\n",
    "\n",
    "        return x.view_as(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        output = grad_output.neg() * ctx.alpha\n",
    "\n",
    "        return output, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (2652268761.py, line 192)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[96], line 192\u001b[0;36m\u001b[0m\n\u001b[0;31m    def test(self):\u001b[0m\n\u001b[0m                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "class Tugda_da(nn.Module):\n",
    "\n",
    "    def __init__(self,params,train_data,test_data,y_train,y_test):\n",
    "        super(Tugda_da,self).__init__()\n",
    "        self.learning_rate = params['lr']\n",
    "        self.batch_size = params['bs_source']\n",
    "        self.mu = params['mu']\n",
    "        self.lambda_ = params['lambda_']\n",
    "        self.gamma = params['gamma']\n",
    "        self.lambda_disc = params['lambda_disc']\n",
    "        self.bs_disc = params['bs_disc']\n",
    "        self.n_epoch = params['n_epochs']\n",
    "        self.passes = params['passes']\n",
    "        self.num_tasks = params['num_tasks']\n",
    "\n",
    "        self.train_data = train_data\n",
    "        self.y_train = y_train\n",
    "        self.test_data = test_data\n",
    "        self.y_test = y_test\n",
    "\n",
    "        input_dim=self.train_data.shape[1]\n",
    "\n",
    "        feature_extractor = [nn.Linear(input_dim, params['hidden_units_1']), \n",
    "                             nn.Dropout(p=params['dropout']),\n",
    "                             nn.ReLU()]\n",
    "        \n",
    "        self.feature_extractor = nn.Sequential(*feature_extractor) #L layer in diag\n",
    "\n",
    "        latent_basis =  [nn.Linear(params['hidden_units_1'], params['latent_space']),\n",
    "                         nn.Dropout(p=params['dropout']),\n",
    "                         nn.ReLU()]\n",
    "        \n",
    "        self.latent_basis = nn.Sequential(*latent_basis) #Z layer in diag\n",
    "        \n",
    "        #task-specific weights\n",
    "        self.MTL = nn.Linear(params['latent_space'], self.num_tasks) #S Layer in diag| also the prediction outputs\n",
    "        \n",
    "        #decoder weights\n",
    "        decTTF = [nn.Linear( self.num_tasks , params['latent_space']), nn.ReLU()] # A layer in diagram or decoder task to feature layer \n",
    "        self.decTTF = nn.Sequential(*decTTF) \n",
    "\n",
    "        #uncertainty (aleatoric)\n",
    "        self.log_vars = torch.zeros(self.num_tasks, requires_grad=True, device=device)\n",
    "        \n",
    "        #discriminator\n",
    "        task_classifier = [nn.Linear(params[\"latent_space\"], params[\"n_units_disc\"]), \n",
    "                             nn.Dropout(p=params['dropout']), \n",
    "                             nn.ReLU(),\n",
    "                             nn.Linear(params[\"n_units_disc\"], 1), \n",
    "                             nn.Sigmoid()\n",
    "                            ]\n",
    "        self.task_classifier = nn.Sequential(*task_classifier)\n",
    "\n",
    "        self.prepare_data()\n",
    "\n",
    "    def forward(self, input_data, alpha):\n",
    "        x = self.feature_extractor(input_data) #usually the input data is treated as x\n",
    "        x = self.latent_basis(x) # here h \n",
    "        preds = self.MTL(x)\n",
    "        x_hat = self.decTTF(preds) # task transfer to features # here was h_hat\n",
    "        reverse_feature = ReverseLayerF.apply(x, alpha)\n",
    "        task_classifier_output = self.task_classifier(reverse_feature)\n",
    "        return preds, x, x_hat, task_classifier_output\n",
    "    \n",
    "    def prepare_data(self):\n",
    "        train_dataset = TensorDataset(torch.FloatTensor(self.train_data),\n",
    "                                      torch.FloatTensor(self.y_train))\n",
    "\n",
    "        \n",
    "        target_unl_dataset = TensorDataset(torch.FloatTensor(self.test_data))\n",
    "        \n",
    "        self.train_dataset = train_dataset\n",
    "        self.target_unl_dataset = target_unl_dataset\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        dataloader1 = DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True) #in vitro data\n",
    "        dataloader2 = DataLoader(self.target_unl_dataset, batch_size=self.bs_disc, shuffle=True) #in vivo data \n",
    "        self.len_dataloader = min(len(dataloader1), len(dataloader2))\n",
    "        x=zip(dataloader1, cycle(dataloader2))\n",
    "        return x\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        dataloader = DataLoader(self.target_unl_dataset, batch_size=len(self.target_unl_dataset), shuffle=False)\n",
    "        return dataloader\n",
    "    \n",
    "    def MSE_loss(self, x, x_hat):\n",
    "        mse_loss = torch.nn.MSELoss()\n",
    "        return mse_loss(x, x_hat)\n",
    "    \n",
    "    def task_mse_ignore_nan(self, preds, labels):\n",
    "        mse_loss = torch.nn.MSELoss(reduction='none')\n",
    "        per_task_loss = torch.zeros(labels.size(1), device=device)\n",
    "\n",
    "        for k in range(labels.size(1)):\n",
    "            precision = torch.exp(-self.log_vars[k]) #didnt understand this.\n",
    "            diff = mse_loss(preds[~torch.isnan(labels[:,k]), k], labels[~torch.isnan(labels[:,k]), k])\n",
    "            per_task_loss[k] = torch.mean(precision * diff + self.log_vars[k])\n",
    "\n",
    "        return torch.mean(per_task_loss[~torch.isnan(per_task_loss)]), per_task_loss\n",
    "\n",
    "    def mse_ignore_nan_test(self, preds, labels):\n",
    "        mse_loss = torch.nn.MSELoss(reduction='mean')\n",
    "        per_task_loss = torch.zeros(labels.size(1), device=device)\n",
    "        per_sample_loss = torch.zeros(labels.size(0), device=device)\n",
    "\n",
    "        \n",
    "        for k in range(labels.size(1)):\n",
    "            per_task_loss[k] = mse_loss(preds[~torch.isnan(labels[:,k]), k], labels[~torch.isnan(labels[:,k]), k])\n",
    "        \n",
    "        #per class loss\n",
    "        for k in range(labels.size(0)):\n",
    "            per_sample_loss[k] = mse_loss(preds[k, ~torch.isnan(labels[k,:])], labels[k, ~torch.isnan(labels[k, :])])\n",
    "        \n",
    "        \n",
    "        return torch.mean(per_task_loss[~torch.isnan(per_task_loss)]), per_task_loss,per_sample_loss\n",
    "    \n",
    "    def binary_classification_loss(self, preds, labels):\n",
    "        bin_loss = torch.nn.BCELoss()\n",
    "        return bin_loss(preds, labels)\n",
    "    \n",
    "    def train(self):\n",
    "\n",
    "        optimizer = torch.optim.Adagrad([\n",
    "            {'params': self.feature_extractor.parameters()},\n",
    "            {'params': self.latent_basis.parameters()},\n",
    "            {'params': self.MTL.parameters()},\n",
    "            {'params': self.decTTF.parameters()},\n",
    "            {'params': self.task_classifier.parameters()},\n",
    "            {'params': self.log_vars}\n",
    "        ], lr=self.learning_rate)\n",
    "        \n",
    "        for epoch in range(self.n_epoch):\n",
    "            train_dataloader=self.train_dataloader()\n",
    "            train_loss=0\n",
    "            for batch_idx,data in enumerate(train_dataloader):\n",
    "                x,y=data[0] #in vitro\n",
    "                x_unl=data[1][0] #invivo\n",
    "\n",
    "                p=float(batch_idx+epoch*self.len_dataloader)/self.n_epoch/self.len_dataloader\n",
    "                alpha=2. / (1. + np.exp(-10 * p)) - 1\n",
    "\n",
    "                preds_simulation=torch.zeros(y.size(0),y.size(1),self.passes)#,device=device) \n",
    "                for simulation in range(self.passes):\n",
    "                    preds, h, h_hat, domain_out_source = self.forward(x, alpha)\n",
    "                    preds_simulation[:,:, simulation]=preds\n",
    "                \n",
    "                preds_mean = torch.mean(preds_simulation, axis=2) #2D predictions after p passes\n",
    "                preds_var = torch.var(preds_simulation, axis=2) #2D\n",
    "                total_unc = torch.mean(preds_var, axis=0) # mu_t this is epistemic uncertainity 1D of tasks i.e y.shape[1]\n",
    "\n",
    "\n",
    "                m_loss, task_loss = self.task_mse_ignore_nan(preds_mean, y)\n",
    "                recon_loss= self.MSE_loss(h,h_hat)\n",
    "\n",
    "                a = 1 + (total_unc + torch.sum(torch.abs(self.decTTF[0].weight.T),1)) #multiplier to L_bnn\n",
    "\n",
    "                loss_weight = ( a[~torch.isnan(task_loss)] ) * task_loss[~torch.isnan(task_loss)]\n",
    "                loss_weight = torch.sum(loss_weight)\n",
    "\n",
    "                #Regularizer gamma(Z-sig(ZSA))\n",
    "                l1_S = self.mu * self.MTL.weight.norm(1)\n",
    "                L = self.latent_basis[0].weight.norm(2) + self.feature_extractor[0].weight.norm(2)\n",
    "                l2_L = self.lambda_ * L\n",
    "\n",
    "                #domain discriminator source\n",
    "                #normal data pass\n",
    "                # 0 is one class of invitro\n",
    "                zeros = torch.zeros(y.size(0),1)#, device=self.device)\n",
    "                d_loss_source = self.binary_classification_loss(domain_out_source, zeros)\n",
    "                \n",
    "                #domain discriminator target\n",
    "                #unlabelled data pass\n",
    "                #1 is second class which is invivo \n",
    "                preds, h, h_hat, domain_out_target = self.forward(x_unl, alpha)\n",
    "                ones = torch.ones(x_unl.size(0),1)#, device=self.device) \n",
    "                d_loss_target = self.binary_classification_loss(domain_out_target, ones)\n",
    "                \n",
    "                d_loss = d_loss_source + d_loss_target #l_adv loss or the adversarial loss \n",
    "\n",
    "                #total loss\n",
    "                loss = loss_weight + recon_loss + l1_S + l2_L + (self.lambda_disc *d_loss)\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                train_loss+=loss\n",
    "\n",
    "            train_loss/=self.len_dataloader\n",
    "            print('Epoch [{}/{}] : Loss {}'.format(epoch,self.n_epoch,train_loss))#, 'source_loss': task_loss, 'disc_loss': d_loss})\n",
    "        return None #loss, m_loss, d_loss\n",
    "    \n",
    "    # def test(self):\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cell-line dataset;\n",
    "gdsc_dataset = pd.read_csv('data/GDSCDA_fpkm_AUC_all_drugs.zip', index_col=0)\n",
    "#gene set range\n",
    "gene_list = gdsc_dataset.columns[0:1780]\n",
    "drug_list = gdsc_dataset.columns[1780:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pdx novartis dataset;\n",
    "pdx_dataset = pd.read_csv('data/PDX_MTL_DA.csv', index_col=0)\n",
    "drugs_pdx = pdx_dataset.columns[1780:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_params = {\n",
    " 'hidden_units_1': 1500,\n",
    " 'latent_space': 800,\n",
    " 'lr': 0.001,\n",
    " 'dropout': 0.1,\n",
    " 'mu': 1,\n",
    " 'lambda_': 1,\n",
    " 'gamma': 0.01,\n",
    " 'n_units_disc': 500,\n",
    " 'n_epochs': 150, #50\n",
    " 'bs_disc': 64,\n",
    " 'bs_source': 300,\n",
    " 'lambda_disc': 0.3,\n",
    " 'num_tasks': 200,\n",
    " 'passes': 20}#50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_list = gdsc_dataset.columns[0:1780]\n",
    "drug_list = gdsc_dataset.columns[1780:]\n",
    "\n",
    "X_train = gdsc_dataset[gene_list].values #In vitro data\n",
    "y_train = gdsc_dataset[drug_list].values\n",
    " \n",
    "X_test = pdx_dataset[gene_list].values #In-Vivo data\n",
    "y_test = pdx_dataset[drugs_pdx].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/150] : Loss 5465.60400390625\n",
      "Epoch [1/150] : Loss 3281.790283203125\n",
      "Epoch [2/150] : Loss 3029.475830078125\n",
      "Epoch [3/150] : Loss 2707.848388671875\n",
      "Epoch [4/150] : Loss 2533.971923828125\n",
      "Epoch [5/150] : Loss 2363.306884765625\n",
      "Epoch [6/150] : Loss 2216.306884765625\n",
      "Epoch [7/150] : Loss 2055.726318359375\n",
      "Epoch [8/150] : Loss 1956.2987060546875\n",
      "Epoch [9/150] : Loss 1844.7503662109375\n",
      "Epoch [10/150] : Loss 1749.3697509765625\n",
      "Epoch [11/150] : Loss 1642.3861083984375\n",
      "Epoch [12/150] : Loss 1558.0196533203125\n",
      "Epoch [13/150] : Loss 1476.8834228515625\n",
      "Epoch [14/150] : Loss 1404.9962158203125\n",
      "Epoch [15/150] : Loss 1337.8436279296875\n",
      "Epoch [16/150] : Loss 1271.7005615234375\n",
      "Epoch [17/150] : Loss 1208.3077392578125\n",
      "Epoch [18/150] : Loss 1151.0533447265625\n",
      "Epoch [19/150] : Loss 1095.324462890625\n",
      "Epoch [20/150] : Loss 1048.3060302734375\n",
      "Epoch [21/150] : Loss 1003.436767578125\n",
      "Epoch [22/150] : Loss 958.6067504882812\n",
      "Epoch [23/150] : Loss 918.3923950195312\n",
      "Epoch [24/150] : Loss 880.2361450195312\n",
      "Epoch [25/150] : Loss 843.3859252929688\n",
      "Epoch [26/150] : Loss 808.9669799804688\n",
      "Epoch [27/150] : Loss 776.82568359375\n",
      "Epoch [28/150] : Loss 748.4953002929688\n",
      "Epoch [29/150] : Loss 722.1992797851562\n",
      "Epoch [30/150] : Loss 698.9406127929688\n",
      "Epoch [31/150] : Loss 674.3585815429688\n",
      "Epoch [32/150] : Loss 652.5498657226562\n",
      "Epoch [33/150] : Loss 632.8495483398438\n",
      "Epoch [34/150] : Loss 613.6083374023438\n",
      "Epoch [35/150] : Loss 596.8435668945312\n",
      "Epoch [36/150] : Loss 580.8004760742188\n",
      "Epoch [37/150] : Loss 566.9657592773438\n",
      "Epoch [38/150] : Loss 555.8228149414062\n",
      "Epoch [39/150] : Loss 543.0100708007812\n",
      "Epoch [40/150] : Loss 532.1581420898438\n",
      "Epoch [41/150] : Loss 524.76513671875\n",
      "Epoch [42/150] : Loss 516.7485961914062\n",
      "Epoch [43/150] : Loss 509.1761779785156\n",
      "Epoch [44/150] : Loss 504.1293640136719\n",
      "Epoch [45/150] : Loss 498.3647766113281\n",
      "Epoch [46/150] : Loss 495.8296813964844\n",
      "Epoch [47/150] : Loss 491.4720764160156\n",
      "Epoch [48/150] : Loss 487.8650817871094\n",
      "Epoch [49/150] : Loss 489.0589294433594\n",
      "Epoch [50/150] : Loss 486.2259826660156\n",
      "Epoch [51/150] : Loss 486.8034973144531\n",
      "Epoch [52/150] : Loss 485.155517578125\n",
      "Epoch [53/150] : Loss 485.3468322753906\n",
      "Epoch [54/150] : Loss 485.8826599121094\n",
      "Epoch [55/150] : Loss 485.5135192871094\n",
      "Epoch [56/150] : Loss 486.5577697753906\n",
      "Epoch [57/150] : Loss 485.0546569824219\n",
      "Epoch [58/150] : Loss 486.986328125\n",
      "Epoch [59/150] : Loss 485.9020080566406\n",
      "Epoch [60/150] : Loss 489.9695739746094\n",
      "Epoch [61/150] : Loss 487.9964599609375\n",
      "Epoch [62/150] : Loss 489.538818359375\n",
      "Epoch [63/150] : Loss 488.5283508300781\n",
      "Epoch [64/150] : Loss 488.9212646484375\n",
      "Epoch [65/150] : Loss 489.1265563964844\n",
      "Epoch [66/150] : Loss 490.046142578125\n",
      "Epoch [67/150] : Loss 489.2657775878906\n",
      "Epoch [68/150] : Loss 488.2564697265625\n",
      "Epoch [69/150] : Loss 486.8994140625\n",
      "Epoch [70/150] : Loss 485.25537109375\n",
      "Epoch [71/150] : Loss 483.6788635253906\n",
      "Epoch [72/150] : Loss 482.7665710449219\n",
      "Epoch [73/150] : Loss 481.5140075683594\n",
      "Epoch [74/150] : Loss 478.5683898925781\n",
      "Epoch [75/150] : Loss 477.2929382324219\n",
      "Epoch [76/150] : Loss 474.1437072753906\n",
      "Epoch [77/150] : Loss 472.6915588378906\n",
      "Epoch [78/150] : Loss 468.789794921875\n",
      "Epoch [79/150] : Loss 467.9745178222656\n",
      "Epoch [80/150] : Loss 462.4130859375\n",
      "Epoch [81/150] : Loss 459.5121765136719\n",
      "Epoch [82/150] : Loss 454.046630859375\n",
      "Epoch [83/150] : Loss 452.7691345214844\n",
      "Epoch [84/150] : Loss 446.9178161621094\n",
      "Epoch [85/150] : Loss 444.6241149902344\n",
      "Epoch [86/150] : Loss 439.36328125\n",
      "Epoch [87/150] : Loss 435.3329772949219\n",
      "Epoch [88/150] : Loss 429.0445251464844\n",
      "Epoch [89/150] : Loss 424.24169921875\n",
      "Epoch [90/150] : Loss 419.4353942871094\n",
      "Epoch [91/150] : Loss 413.242919921875\n",
      "Epoch [92/150] : Loss 409.391845703125\n",
      "Epoch [93/150] : Loss 401.3622741699219\n",
      "Epoch [94/150] : Loss 397.4296569824219\n",
      "Epoch [95/150] : Loss 389.8644104003906\n",
      "Epoch [96/150] : Loss 385.8440856933594\n",
      "Epoch [97/150] : Loss 378.6376953125\n",
      "Epoch [98/150] : Loss 373.4764099121094\n",
      "Epoch [99/150] : Loss 366.6658630371094\n",
      "Epoch [100/150] : Loss 359.9102478027344\n",
      "Epoch [101/150] : Loss 353.1036376953125\n",
      "Epoch [102/150] : Loss 345.9547424316406\n",
      "Epoch [103/150] : Loss 340.638671875\n",
      "Epoch [104/150] : Loss 332.3343811035156\n",
      "Epoch [105/150] : Loss 327.453857421875\n",
      "Epoch [106/150] : Loss 319.5190124511719\n",
      "Epoch [107/150] : Loss 311.5259094238281\n",
      "Epoch [108/150] : Loss 307.2037048339844\n",
      "Epoch [109/150] : Loss 299.124755859375\n",
      "Epoch [110/150] : Loss 292.7008056640625\n",
      "Epoch [111/150] : Loss 283.999755859375\n",
      "Epoch [112/150] : Loss 277.7050476074219\n",
      "Epoch [113/150] : Loss 269.5787658691406\n",
      "Epoch [114/150] : Loss 260.6874084472656\n",
      "Epoch [115/150] : Loss 256.0636291503906\n",
      "Epoch [116/150] : Loss 247.465087890625\n",
      "Epoch [117/150] : Loss 240.62525939941406\n",
      "Epoch [118/150] : Loss 233.72984313964844\n",
      "Epoch [119/150] : Loss 224.7256622314453\n",
      "Epoch [120/150] : Loss 218.3479461669922\n",
      "Epoch [121/150] : Loss 211.4318389892578\n",
      "Epoch [122/150] : Loss 202.66770935058594\n",
      "Epoch [123/150] : Loss 197.2026824951172\n",
      "Epoch [124/150] : Loss 188.1121063232422\n",
      "Epoch [125/150] : Loss 180.9525909423828\n",
      "Epoch [126/150] : Loss 175.18519592285156\n",
      "Epoch [127/150] : Loss 166.87059020996094\n",
      "Epoch [128/150] : Loss 159.4578857421875\n",
      "Epoch [129/150] : Loss 153.41273498535156\n",
      "Epoch [130/150] : Loss 143.34617614746094\n",
      "Epoch [131/150] : Loss 137.81564331054688\n",
      "Epoch [132/150] : Loss 131.83531188964844\n",
      "Epoch [133/150] : Loss 121.33649444580078\n",
      "Epoch [134/150] : Loss 116.12297821044922\n",
      "Epoch [135/150] : Loss 109.84465789794922\n",
      "Epoch [136/150] : Loss 99.18827056884766\n",
      "Epoch [137/150] : Loss 94.2814712524414\n",
      "Epoch [138/150] : Loss 88.82308197021484\n",
      "Epoch [139/150] : Loss 77.58406829833984\n",
      "Epoch [140/150] : Loss 72.55199432373047\n",
      "Epoch [141/150] : Loss 66.97726440429688\n",
      "Epoch [142/150] : Loss 57.67340087890625\n",
      "Epoch [143/150] : Loss 50.79899215698242\n",
      "Epoch [144/150] : Loss 45.79085922241211\n",
      "Epoch [145/150] : Loss 36.25074768066406\n",
      "Epoch [146/150] : Loss 29.763389587402344\n",
      "Epoch [147/150] : Loss 25.0074462890625\n",
      "Epoch [148/150] : Loss 15.953730583190918\n",
      "Epoch [149/150] : Loss 9.402484893798828\n"
     ]
    }
   ],
   "source": [
    "model = Tugda_da(net_params, X_train, X_test, y_train, y_test)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
