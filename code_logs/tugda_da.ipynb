{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4fc2bf98",
   "metadata": {},
   "outputs": [],
   "source": [
    "%notebook logs/tugda_da.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95e40463",
   "metadata": {},
   "outputs": [],
   "source": [
    "%notebook logs/tugda_da.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13a2b277",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tugda_da(nn.Module):\n",
    "\n",
    "    def __init__(self,params,train_data,test_data,y_train,y_test):\n",
    "        super(Tugda_da,self).__init__()\n",
    "        self.learning_rate = params['lr']\n",
    "        self.batch_size = params['bs_source']\n",
    "        self.mu = params['mu']\n",
    "        self.lambda_ = params['lambda_']\n",
    "        self.gamma = params['gamma']\n",
    "        self.lambda_disc = params['lambda_disc']\n",
    "        self.bs_disc = params['bs_disc']\n",
    "        self.n_epoch = params['n_epochs']\n",
    "        self.passes = params['passes']\n",
    "        self.num_tasks = params['num_tasks']\n",
    "\n",
    "        self.train_data = train_data\n",
    "        self.y_train = y_train\n",
    "        self.test_data = test_data\n",
    "        self.y_test = y_test\n",
    "\n",
    "        input_dim=self.train_data.shape[1]\n",
    "\n",
    "        feature_extractor = [nn.Linear(input_dim, params['hidden_units_1']), \n",
    "                             nn.Dropout(p=params['dropout']),\n",
    "                             nn.ReLU()]\n",
    "        \n",
    "        self.feature_extractor = nn.Sequential(*feature_extractor) #L layer in diag\n",
    "\n",
    "        latent_basis =  [nn.Linear(params['hidden_units_1'], params['latent_space']),\n",
    "                         nn.Dropout(p=params['dropout']),\n",
    "                         nn.ReLU()]\n",
    "        \n",
    "        self.latent_basis = nn.Sequential(*latent_basis) #Z layer in diag\n",
    "        \n",
    "        #task-specific weights\n",
    "        self.MTL = nn.Linear(params['latent_space'], self.num_tasks) #S Layer in diag| also the prediction outputs\n",
    "        \n",
    "        #decoder weights\n",
    "        decTTF = [nn.Linear( self.num_tasks , params['latent_space']), nn.ReLU()] # A layer in diag or decoder task to feature layer \n",
    "        self.decTTF = nn.Sequential(*decTTF) \n",
    "\n",
    "        #uncertainty (aleatoric)\n",
    "        self.log_vars = torch.zeros(self.num_tasks, requires_grad=True, device=device)\n",
    "        \n",
    "        #discriminator\n",
    "        task_classifier = [nn.Linear(params[\"latent_space\"], params[\"n_units_disc\"]), \n",
    "                             nn.Dropout(p=params['dropout']), \n",
    "                             nn.ReLU(),\n",
    "                             nn.Linear(params[\"n_units_disc\"], 1), \n",
    "                             nn.Sigmoid()\n",
    "                            ]\n",
    "        self.task_classifier = nn.Sequential(*task_classifier)\n",
    "\n",
    "    def forward(self, input_data, alpha):\n",
    "        x = self.feature_extractor(input_data) #usually the input data is treated as x\n",
    "        x = self.latent_basis(x) # here h \n",
    "        preds = self.MTL(x)\n",
    "        x_hat = self.decTTF(preds) # task transfer to features # here was h_hat\n",
    "        reverse_feature = ReverseLayerF.apply(x, alpha)\n",
    "        task_classifier_output = self.task_classifier(reverse_feature)\n",
    "        return preds, x, x_hat, task_classifier_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3702c670",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from itertools import cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e295ff0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.autograd import Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f55f3670",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc982b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tugda_da(nn.Module):\n",
    "\n",
    "    def __init__(self,params,train_data,test_data,y_train,y_test):\n",
    "        super(Tugda_da,self).__init__()\n",
    "        self.learning_rate = params['lr']\n",
    "        self.batch_size = params['bs_source']\n",
    "        self.mu = params['mu']\n",
    "        self.lambda_ = params['lambda_']\n",
    "        self.gamma = params['gamma']\n",
    "        self.lambda_disc = params['lambda_disc']\n",
    "        self.bs_disc = params['bs_disc']\n",
    "        self.n_epoch = params['n_epochs']\n",
    "        self.passes = params['passes']\n",
    "        self.num_tasks = params['num_tasks']\n",
    "\n",
    "        self.train_data = train_data\n",
    "        self.y_train = y_train\n",
    "        self.test_data = test_data\n",
    "        self.y_test = y_test\n",
    "\n",
    "        input_dim=self.train_data.shape[1]\n",
    "\n",
    "        feature_extractor = [nn.Linear(input_dim, params['hidden_units_1']), \n",
    "                             nn.Dropout(p=params['dropout']),\n",
    "                             nn.ReLU()]\n",
    "        \n",
    "        self.feature_extractor = nn.Sequential(*feature_extractor) #L layer in diag\n",
    "\n",
    "        latent_basis =  [nn.Linear(params['hidden_units_1'], params['latent_space']),\n",
    "                         nn.Dropout(p=params['dropout']),\n",
    "                         nn.ReLU()]\n",
    "        \n",
    "        self.latent_basis = nn.Sequential(*latent_basis) #Z layer in diag\n",
    "        \n",
    "        #task-specific weights\n",
    "        self.MTL = nn.Linear(params['latent_space'], self.num_tasks) #S Layer in diag| also the prediction outputs\n",
    "        \n",
    "        #decoder weights\n",
    "        decTTF = [nn.Linear( self.num_tasks , params['latent_space']), nn.ReLU()] # A layer in diag or decoder task to feature layer \n",
    "        self.decTTF = nn.Sequential(*decTTF) \n",
    "\n",
    "        #uncertainty (aleatoric)\n",
    "        self.log_vars = torch.zeros(self.num_tasks, requires_grad=True, device=device)\n",
    "        \n",
    "        #discriminator\n",
    "        task_classifier = [nn.Linear(params[\"latent_space\"], params[\"n_units_disc\"]), \n",
    "                             nn.Dropout(p=params['dropout']), \n",
    "                             nn.ReLU(),\n",
    "                             nn.Linear(params[\"n_units_disc\"], 1), \n",
    "                             nn.Sigmoid()\n",
    "                            ]\n",
    "        self.task_classifier = nn.Sequential(*task_classifier)\n",
    "\n",
    "    def forward(self, input_data, alpha):\n",
    "        x = self.feature_extractor(input_data) #usually the input data is treated as x\n",
    "        x = self.latent_basis(x) # here h \n",
    "        preds = self.MTL(x)\n",
    "        x_hat = self.decTTF(preds) # task transfer to features # here was h_hat\n",
    "        reverse_feature = ReverseLayerF.apply(x, alpha)\n",
    "        task_classifier_output = self.task_classifier(reverse_feature)\n",
    "        return preds, x, x_hat, task_classifier_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f2e7ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=torch.zeros(3,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c9b9759",
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a942b4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=torch.rand(5,5,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1fcfdd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=torch.rand(5,5,4)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d4894562",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_mean = torch.mean(y, axis=2)\n",
    "preds_var = torch.var(y, axis=2)\n",
    "total_unc = torch.mean(preds_var, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a3bb9c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(preds_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eba1ea8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(preds_mean)\n",
    "print(preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bed6ef5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(preds_mean)\n",
    "print(preds_mean.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "319e1362",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(preds_var)\n",
    "print(preds_var.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "739b1f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(total_unc)\n",
    "print(total_unc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "00f7df3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=torch.rand(5,6,4)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "575c97e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_mean = torch.mean(y, axis=2)\n",
    "preds_var = torch.var(y, axis=2)\n",
    "total_unc = torch.mean(preds_var, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "491509e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(preds_mean)\n",
    "print(preds_mean.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "90f7384a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(preds_var)\n",
    "print(preds_var.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ef2b82b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(total_unc)\n",
    "print(total_unc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d6abb682",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cell-line dataset;\n",
    "gdsc_dataset = pd.read_csv('data/GDSCDA_fpkm_AUC_all_drugs.zip', index_col=0)\n",
    "#gene set range\n",
    "gene_list = gdsc_dataset.columns[0:1780]\n",
    "drug_list = gdsc_dataset.columns[1780:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f13da3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pdx novartis dataset;\n",
    "pdx_dataset = pd.read_csv('data/PDX_MTL_DA.csv', index_col=0)\n",
    "drugs_pdx = pdx_dataset.columns[1780:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bf6eed7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_params = {\n",
    " 'hidden_units_1': 1500,\n",
    " 'latent_space': 800,\n",
    " 'lr': 0.001,\n",
    " 'dropout': 0.1,\n",
    " 'mu': 1,\n",
    " 'lambda_': 1,\n",
    " 'gamma': 0.01,\n",
    " 'n_units_disc': 500,\n",
    " 'n_epochs': 50,\n",
    " 'bs_disc': 64,\n",
    " 'bs_source': 300,\n",
    " 'lambda_disc': 0.3,\n",
    " 'num_tasks': 200,\n",
    " 'passes': 50}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c21863b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_list = gdsc_dataset.columns[0:1780]\n",
    "drug_list = gdsc_dataset.columns[1780:]\n",
    "\n",
    "X_train = gdsc_dataset[gene_list].values\n",
    "y_train = gdsc_dataset[drug_list].values\n",
    "\n",
    "X_test = pdx_dataset[gene_list].values\n",
    "y_test = pdx_dataset[drugs_pdx].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "88638da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tugda_da(nn.Module):\n",
    "\n",
    "    def __init__(self,params,train_data,test_data,y_train,y_test):\n",
    "        super(Tugda_da,self).__init__()\n",
    "        self.learning_rate = params['lr']\n",
    "        self.batch_size = params['bs_source']\n",
    "        self.mu = params['mu']\n",
    "        self.lambda_ = params['lambda_']\n",
    "        self.gamma = params['gamma']\n",
    "        self.lambda_disc = params['lambda_disc']\n",
    "        self.bs_disc = params['bs_disc']\n",
    "        self.n_epoch = params['n_epochs']\n",
    "        self.passes = params['passes']\n",
    "        self.num_tasks = params['num_tasks']\n",
    "\n",
    "        self.train_data = train_data\n",
    "        self.y_train = y_train\n",
    "        self.test_data = test_data\n",
    "        self.y_test = y_test\n",
    "\n",
    "        input_dim=self.train_data.shape[1]\n",
    "\n",
    "        feature_extractor = [nn.Linear(input_dim, params['hidden_units_1']), \n",
    "                             nn.Dropout(p=params['dropout']),\n",
    "                             nn.ReLU()]\n",
    "        \n",
    "        self.feature_extractor = nn.Sequential(*feature_extractor) #L layer in diag\n",
    "\n",
    "        latent_basis =  [nn.Linear(params['hidden_units_1'], params['latent_space']),\n",
    "                         nn.Dropout(p=params['dropout']),\n",
    "                         nn.ReLU()]\n",
    "        \n",
    "        self.latent_basis = nn.Sequential(*latent_basis) #Z layer in diag\n",
    "        \n",
    "        #task-specific weights\n",
    "        self.MTL = nn.Linear(params['latent_space'], self.num_tasks) #S Layer in diag| also the prediction outputs\n",
    "        \n",
    "        #decoder weights\n",
    "        decTTF = [nn.Linear( self.num_tasks , params['latent_space']), nn.ReLU()] # A layer in diag or decoder task to feature layer \n",
    "        self.decTTF = nn.Sequential(*decTTF) \n",
    "\n",
    "        #uncertainty (aleatoric)\n",
    "        self.log_vars = torch.zeros(self.num_tasks, requires_grad=True, device=device)\n",
    "        \n",
    "        #discriminator\n",
    "        task_classifier = [nn.Linear(params[\"latent_space\"], params[\"n_units_disc\"]), \n",
    "                             nn.Dropout(p=params['dropout']), \n",
    "                             nn.ReLU(),\n",
    "                             nn.Linear(params[\"n_units_disc\"], 1), \n",
    "                             nn.Sigmoid()\n",
    "                            ]\n",
    "        self.task_classifier = nn.Sequential(*task_classifier)\n",
    "\n",
    "    def forward(self, input_data, alpha):\n",
    "        x = self.feature_extractor(input_data) #usually the input data is treated as x\n",
    "        x = self.latent_basis(x) # here h \n",
    "        preds = self.MTL(x)\n",
    "        x_hat = self.decTTF(preds) # task transfer to features # here was h_hat\n",
    "        reverse_feature = ReverseLayerF.apply(x, alpha)\n",
    "        task_classifier_output = self.task_classifier(reverse_feature)\n",
    "        return preds, x, x_hat, task_classifier_output\n",
    "    \n",
    "    def prepare_data(self):\n",
    "        train_dataset = TensorDataset(torch.FloatTensor(self.train_data),\n",
    "                                      torch.FloatTensor(self.y_train))\n",
    "\n",
    "        \n",
    "        target_unl_dataset = TensorDataset(torch.FloatTensor(self.x_target_unl))\n",
    "        \n",
    "        self.train_dataset = train_dataset\n",
    "        self.target_unl_dataset = target_unl_dataset\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        dataloader1 = DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True) #in vitro data\n",
    "        dataloader2 = DataLoader(self.target_unl_dataset, batch_size=self.bs_disc, shuffle=True) #in vivo data \n",
    "        self.len_dataloader = min(len(dataloader1), len(dataloader2))\n",
    "        x=zip(dataloader1, cycle(dataloader2))\n",
    "        return x\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        dataloader = DataLoader(self.target_unl_dataset, batch_size=len(self.target_unl_dataset), shuffle=False)\n",
    "        return dataloader\n",
    "    \n",
    "    def MSE_loss(self, x, x_hat):\n",
    "        mse_loss = torch.nn.MSELoss()\n",
    "        return mse_loss(x, x_hat)\n",
    "    \n",
    "    def task_mse_ignore_nan(self, preds, labels):\n",
    "        mse_loss = torch.nn.MSELoss(reduction='none')\n",
    "        per_task_loss = torch.zeros(labels.size(1), device=device)\n",
    "\n",
    "        for k in range(labels.size(1)):\n",
    "            precision = torch.exp(-self.log_vars[k]) #didnt understand this.\n",
    "            diff = mse_loss(preds[~torch.isnan(labels[:,k]), k], labels[~torch.isnan(labels[:,k]), k])\n",
    "            per_task_loss[k] = torch.mean(precision * diff + self.log_vars[k])\n",
    "\n",
    "        return torch.mean(per_task_loss[~torch.isnan(per_task_loss)]), per_task_loss\n",
    "\n",
    "    def mse_ignore_nan_test(self, preds, labels):\n",
    "        mse_loss = torch.nn.MSELoss(reduction='mean')\n",
    "        per_task_loss = torch.zeros(labels.size(1), device=device)\n",
    "        per_sample_loss = torch.zeros(labels.size(0), device=device)\n",
    "\n",
    "        \n",
    "        for k in range(labels.size(1)):\n",
    "            per_task_loss[k] = mse_loss(preds[~torch.isnan(labels[:,k]), k], labels[~torch.isnan(labels[:,k]), k])\n",
    "        \n",
    "        #per class loss\n",
    "        for k in range(labels.size(0)):\n",
    "            per_sample_loss[k] = mse_loss(preds[k, ~torch.isnan(labels[k,:])], labels[k, ~torch.isnan(labels[k, :])])\n",
    "        \n",
    "        \n",
    "        return torch.mean(per_task_loss[~torch.isnan(per_task_loss)]), per_task_loss,per_sample_loss\n",
    "    \n",
    "    def binary_classification_loss(self, preds, labels):\n",
    "        bin_loss = torch.nn.BCELoss()\n",
    "        return bin_loss(preds, labels)\n",
    "    \n",
    "    def train(self):\n",
    "\n",
    "        optimizer = torch.optim.Adagrad([\n",
    "            {'params': self.feature_extractor.parameters()},\n",
    "            {'params': self.latent_basis.parameters()},\n",
    "            {'params': self.MTL.parameters()},\n",
    "            {'params': self.decTTF.parameters()},\n",
    "            {'params': self.task_classifier.parameters()},\n",
    "            {'params': self.log_vars}\n",
    "        ], lr=self.learning_rate)\n",
    "\n",
    "        for epoch in range(self.n_epoch):\n",
    "            for batch_idx,data in enumerate(self.train_dataloader):\n",
    "                x,y=data[0] #in vitro\n",
    "                x_unl=data[1][0] #invivo\n",
    "\n",
    "                p=float(batch_idx+epoch*self.len_dataloader)/self.n_epoch/self.len_dataloader\n",
    "                alpha=2. / (1. + np.exp(-10 * p)) - 1\n",
    "\n",
    "                preds_simulation=torch.zeros(y.size(0),y.size(1),self.passes,device=device) \n",
    "                for simulation in range(self.passes):\n",
    "                    preds, h, h_hat, domain_out_source = self.forward(x, alpha)\n",
    "                    preds_simulation[:,:, simulation]=preds\n",
    "                \n",
    "                preds_mean = torch.mean(preds_simulation, axis=2) #2D predictions after p passes\n",
    "                preds_var = torch.var(preds_simulation, axis=2) #2D\n",
    "                total_unc = torch.mean(preds_var, axis=0) # mu_t this is epistemic uncertainity 1D of tasks i.e y.shape[1]\n",
    "\n",
    "\n",
    "                m_loss, task_loss = self.task_mse_ignore_nan(preds_mean, y)\n",
    "                recon_loss= self.MSE_loss(h,h_hat)\n",
    "\n",
    "                a = 1 + (total_unc + torch.sum(torch.abs(self.decTTF[0].weight.T),1)) #multiplier to L_bnn\n",
    "\n",
    "                loss_weight = ( a[~torch.isnan(task_loss)] ) * task_loss[~torch.isnan(task_loss)]\n",
    "                loss_weight = torch.sum(loss_weight)\n",
    "\n",
    "                #Regularizer gamma(Z-sig(ZSA))\n",
    "                l1_S = self.mu * self.MTL.weight.norm(1)\n",
    "                L = self.latent_basis[0].weight.norm(2) + self.feature_extractor[0].weight.norm(2)\n",
    "                l2_L = self.lambda_ * L\n",
    "\n",
    "                #domain discriminator source\n",
    "                #normal data pass\n",
    "                # 0 is one class of invitro\n",
    "                zeros = torch.zeros(y.size(0), device=self.device)\n",
    "                d_loss_source = self.binary_classification_loss(domain_out_source, zeros)\n",
    "                \n",
    "                #domain discriminator target\n",
    "                #unlabelled data pass\n",
    "                #1 is second class which is invivo \n",
    "                preds, h, h_hat, domain_out_target = self.forward(x_unl, alpha)\n",
    "                ones = torch.ones(x_unl.size(0), device=self.device) \n",
    "                d_loss_target = self.binary_classification_loss(domain_out_target, ones)\n",
    "                \n",
    "                d_loss = d_loss_source + d_loss_target #l_adv loss or the adversarial loss \n",
    "\n",
    "                #total loss\n",
    "                loss = loss_weight + recon_loss + l1_S + l2_L + (self.lambda_disc *d_loss)\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                print({'total_loss': loss, 'source_loss': task_loss, 'disc_loss': d_loss})\n",
    "\n",
    "                return loss, m_loss, d_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c67d26ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Tugda_da(net_params, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ffeb54df",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_params = {\n",
    " 'hidden_units_1': 1500,\n",
    " 'latent_space': 800,\n",
    " 'lr': 0.001,\n",
    " 'dropout': 0.1,\n",
    " 'mu': 1,\n",
    " 'lambda_': 1,\n",
    " 'gamma': 0.01,\n",
    " 'n_units_disc': 500,\n",
    " 'n_epochs': 5,\n",
    " 'bs_disc': 64,\n",
    " 'bs_source': 300,\n",
    " 'lambda_disc': 0.3,\n",
    " 'num_tasks': 200,\n",
    " 'passes': 50}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "13963776",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_params = {\n",
    " 'hidden_units_1': 1500,\n",
    " 'latent_space': 800,\n",
    " 'lr': 0.001,\n",
    " 'dropout': 0.1,\n",
    " 'mu': 1,\n",
    " 'lambda_': 1,\n",
    " 'gamma': 0.01,\n",
    " 'n_units_disc': 500,\n",
    " 'n_epochs': 5,\n",
    " 'bs_disc': 64,\n",
    " 'bs_source': 300,\n",
    " 'lambda_disc': 0.3,\n",
    " 'num_tasks': 200,\n",
    " 'passes': 10}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d7e16d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tugda_da(nn.Module):\n",
    "\n",
    "    def __init__(self,params,train_data,test_data,y_train,y_test):\n",
    "        super(Tugda_da,self).__init__()\n",
    "        self.learning_rate = params['lr']\n",
    "        self.batch_size = params['bs_source']\n",
    "        self.mu = params['mu']\n",
    "        self.lambda_ = params['lambda_']\n",
    "        self.gamma = params['gamma']\n",
    "        self.lambda_disc = params['lambda_disc']\n",
    "        self.bs_disc = params['bs_disc']\n",
    "        self.n_epoch = params['n_epochs']\n",
    "        self.passes = params['passes']\n",
    "        self.num_tasks = params['num_tasks']\n",
    "\n",
    "        self.train_data = train_data\n",
    "        self.y_train = y_train\n",
    "        self.test_data = test_data\n",
    "        self.y_test = y_test\n",
    "\n",
    "        input_dim=self.train_data.shape[1]\n",
    "\n",
    "        feature_extractor = [nn.Linear(input_dim, params['hidden_units_1']), \n",
    "                             nn.Dropout(p=params['dropout']),\n",
    "                             nn.ReLU()]\n",
    "        \n",
    "        self.feature_extractor = nn.Sequential(*feature_extractor) #L layer in diag\n",
    "\n",
    "        latent_basis =  [nn.Linear(params['hidden_units_1'], params['latent_space']),\n",
    "                         nn.Dropout(p=params['dropout']),\n",
    "                         nn.ReLU()]\n",
    "        \n",
    "        self.latent_basis = nn.Sequential(*latent_basis) #Z layer in diag\n",
    "        \n",
    "        #task-specific weights\n",
    "        self.MTL = nn.Linear(params['latent_space'], self.num_tasks) #S Layer in diag| also the prediction outputs\n",
    "        \n",
    "        #decoder weights\n",
    "        decTTF = [nn.Linear( self.num_tasks , params['latent_space']), nn.ReLU()] # A layer in diag or decoder task to feature layer \n",
    "        self.decTTF = nn.Sequential(*decTTF) \n",
    "\n",
    "        #uncertainty (aleatoric)\n",
    "        self.log_vars = torch.zeros(self.num_tasks, requires_grad=True, device=device)\n",
    "        \n",
    "        #discriminator\n",
    "        task_classifier = [nn.Linear(params[\"latent_space\"], params[\"n_units_disc\"]), \n",
    "                             nn.Dropout(p=params['dropout']), \n",
    "                             nn.ReLU(),\n",
    "                             nn.Linear(params[\"n_units_disc\"], 1), \n",
    "                             nn.Sigmoid()\n",
    "                            ]\n",
    "        self.task_classifier = nn.Sequential(*task_classifier)\n",
    "\n",
    "    def forward(self, input_data, alpha):\n",
    "        x = self.feature_extractor(input_data) #usually the input data is treated as x\n",
    "        x = self.latent_basis(x) # here h \n",
    "        preds = self.MTL(x)\n",
    "        x_hat = self.decTTF(preds) # task transfer to features # here was h_hat\n",
    "        reverse_feature = ReverseLayerF.apply(x, alpha)\n",
    "        task_classifier_output = self.task_classifier(reverse_feature)\n",
    "        return preds, x, x_hat, task_classifier_output\n",
    "    \n",
    "    def prepare_data(self):\n",
    "        train_dataset = TensorDataset(torch.FloatTensor(self.train_data),\n",
    "                                      torch.FloatTensor(self.y_train))\n",
    "\n",
    "        \n",
    "        target_unl_dataset = TensorDataset(torch.FloatTensor(self.test_data))\n",
    "        \n",
    "        self.train_dataset = train_dataset\n",
    "        self.target_unl_dataset = target_unl_dataset\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        dataloader1 = DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True) #in vitro data\n",
    "        dataloader2 = DataLoader(self.target_unl_dataset, batch_size=self.bs_disc, shuffle=True) #in vivo data \n",
    "        self.len_dataloader = min(len(dataloader1), len(dataloader2))\n",
    "        x=zip(dataloader1, cycle(dataloader2))\n",
    "        return x\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        dataloader = DataLoader(self.target_unl_dataset, batch_size=len(self.target_unl_dataset), shuffle=False)\n",
    "        return dataloader\n",
    "    \n",
    "    def MSE_loss(self, x, x_hat):\n",
    "        mse_loss = torch.nn.MSELoss()\n",
    "        return mse_loss(x, x_hat)\n",
    "    \n",
    "    def task_mse_ignore_nan(self, preds, labels):\n",
    "        mse_loss = torch.nn.MSELoss(reduction='none')\n",
    "        per_task_loss = torch.zeros(labels.size(1), device=device)\n",
    "\n",
    "        for k in range(labels.size(1)):\n",
    "            precision = torch.exp(-self.log_vars[k]) #didnt understand this.\n",
    "            diff = mse_loss(preds[~torch.isnan(labels[:,k]), k], labels[~torch.isnan(labels[:,k]), k])\n",
    "            per_task_loss[k] = torch.mean(precision * diff + self.log_vars[k])\n",
    "\n",
    "        return torch.mean(per_task_loss[~torch.isnan(per_task_loss)]), per_task_loss\n",
    "\n",
    "    def mse_ignore_nan_test(self, preds, labels):\n",
    "        mse_loss = torch.nn.MSELoss(reduction='mean')\n",
    "        per_task_loss = torch.zeros(labels.size(1), device=device)\n",
    "        per_sample_loss = torch.zeros(labels.size(0), device=device)\n",
    "\n",
    "        \n",
    "        for k in range(labels.size(1)):\n",
    "            per_task_loss[k] = mse_loss(preds[~torch.isnan(labels[:,k]), k], labels[~torch.isnan(labels[:,k]), k])\n",
    "        \n",
    "        #per class loss\n",
    "        for k in range(labels.size(0)):\n",
    "            per_sample_loss[k] = mse_loss(preds[k, ~torch.isnan(labels[k,:])], labels[k, ~torch.isnan(labels[k, :])])\n",
    "        \n",
    "        \n",
    "        return torch.mean(per_task_loss[~torch.isnan(per_task_loss)]), per_task_loss,per_sample_loss\n",
    "    \n",
    "    def binary_classification_loss(self, preds, labels):\n",
    "        bin_loss = torch.nn.BCELoss()\n",
    "        return bin_loss(preds, labels)\n",
    "    \n",
    "    def train(self):\n",
    "\n",
    "        optimizer = torch.optim.Adagrad([\n",
    "            {'params': self.feature_extractor.parameters()},\n",
    "            {'params': self.latent_basis.parameters()},\n",
    "            {'params': self.MTL.parameters()},\n",
    "            {'params': self.decTTF.parameters()},\n",
    "            {'params': self.task_classifier.parameters()},\n",
    "            {'params': self.log_vars}\n",
    "        ], lr=self.learning_rate)\n",
    "\n",
    "        for epoch in range(self.n_epoch):\n",
    "            for batch_idx,data in enumerate(self.train_dataloader):\n",
    "                x,y=data[0] #in vitro\n",
    "                x_unl=data[1][0] #invivo\n",
    "\n",
    "                p=float(batch_idx+epoch*self.len_dataloader)/self.n_epoch/self.len_dataloader\n",
    "                alpha=2. / (1. + np.exp(-10 * p)) - 1\n",
    "\n",
    "                preds_simulation=torch.zeros(y.size(0),y.size(1),self.passes,device=device) \n",
    "                for simulation in range(self.passes):\n",
    "                    preds, h, h_hat, domain_out_source = self.forward(x, alpha)\n",
    "                    preds_simulation[:,:, simulation]=preds\n",
    "                \n",
    "                preds_mean = torch.mean(preds_simulation, axis=2) #2D predictions after p passes\n",
    "                preds_var = torch.var(preds_simulation, axis=2) #2D\n",
    "                total_unc = torch.mean(preds_var, axis=0) # mu_t this is epistemic uncertainity 1D of tasks i.e y.shape[1]\n",
    "\n",
    "\n",
    "                m_loss, task_loss = self.task_mse_ignore_nan(preds_mean, y)\n",
    "                recon_loss= self.MSE_loss(h,h_hat)\n",
    "\n",
    "                a = 1 + (total_unc + torch.sum(torch.abs(self.decTTF[0].weight.T),1)) #multiplier to L_bnn\n",
    "\n",
    "                loss_weight = ( a[~torch.isnan(task_loss)] ) * task_loss[~torch.isnan(task_loss)]\n",
    "                loss_weight = torch.sum(loss_weight)\n",
    "\n",
    "                #Regularizer gamma(Z-sig(ZSA))\n",
    "                l1_S = self.mu * self.MTL.weight.norm(1)\n",
    "                L = self.latent_basis[0].weight.norm(2) + self.feature_extractor[0].weight.norm(2)\n",
    "                l2_L = self.lambda_ * L\n",
    "\n",
    "                #domain discriminator source\n",
    "                #normal data pass\n",
    "                # 0 is one class of invitro\n",
    "                zeros = torch.zeros(y.size(0), device=self.device)\n",
    "                d_loss_source = self.binary_classification_loss(domain_out_source, zeros)\n",
    "                \n",
    "                #domain discriminator target\n",
    "                #unlabelled data pass\n",
    "                #1 is second class which is invivo \n",
    "                preds, h, h_hat, domain_out_target = self.forward(x_unl, alpha)\n",
    "                ones = torch.ones(x_unl.size(0), device=self.device) \n",
    "                d_loss_target = self.binary_classification_loss(domain_out_target, ones)\n",
    "                \n",
    "                d_loss = d_loss_source + d_loss_target #l_adv loss or the adversarial loss \n",
    "\n",
    "                #total loss\n",
    "                loss = loss_weight + recon_loss + l1_S + l2_L + (self.lambda_disc *d_loss)\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                print({'total_loss': loss, 'source_loss': task_loss, 'disc_loss': d_loss})\n",
    "\n",
    "        return None #loss, m_loss, d_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "89b583db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cell-line dataset;\n",
    "gdsc_dataset = pd.read_csv('data/GDSCDA_fpkm_AUC_all_drugs.zip', index_col=0)\n",
    "#gene set range\n",
    "gene_list = gdsc_dataset.columns[0:1780]\n",
    "drug_list = gdsc_dataset.columns[1780:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d63dc28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pdx novartis dataset;\n",
    "pdx_dataset = pd.read_csv('data/PDX_MTL_DA.csv', index_col=0)\n",
    "drugs_pdx = pdx_dataset.columns[1780:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "75973525",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_params = {\n",
    " 'hidden_units_1': 1500,\n",
    " 'latent_space': 800,\n",
    " 'lr': 0.001,\n",
    " 'dropout': 0.1,\n",
    " 'mu': 1,\n",
    " 'lambda_': 1,\n",
    " 'gamma': 0.01,\n",
    " 'n_units_disc': 500,\n",
    " 'n_epochs': 5, #50\n",
    " 'bs_disc': 64,\n",
    " 'bs_source': 300,\n",
    " 'lambda_disc': 0.3,\n",
    " 'num_tasks': 200,\n",
    " 'passes': 10}#50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0f45118c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_list = gdsc_dataset.columns[0:1780]\n",
    "drug_list = gdsc_dataset.columns[1780:]\n",
    "\n",
    "X_train = gdsc_dataset[gene_list].values\n",
    "y_train = gdsc_dataset[drug_list].values\n",
    "\n",
    "X_test = pdx_dataset[gene_list].values\n",
    "y_test = pdx_dataset[drugs_pdx].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "09ca8d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Tugda_da(net_params, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "aab31b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3fd06e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tugda_da(nn.Module):\n",
    "\n",
    "    def __init__(self,params,train_data,test_data,y_train,y_test):\n",
    "        super(Tugda_da,self).__init__()\n",
    "        self.learning_rate = params['lr']\n",
    "        self.batch_size = params['bs_source']\n",
    "        self.mu = params['mu']\n",
    "        self.lambda_ = params['lambda_']\n",
    "        self.gamma = params['gamma']\n",
    "        self.lambda_disc = params['lambda_disc']\n",
    "        self.bs_disc = params['bs_disc']\n",
    "        self.n_epoch = params['n_epochs']\n",
    "        self.passes = params['passes']\n",
    "        self.num_tasks = params['num_tasks']\n",
    "\n",
    "        self.train_data = train_data\n",
    "        self.y_train = y_train\n",
    "        self.test_data = test_data\n",
    "        self.y_test = y_test\n",
    "\n",
    "        input_dim=self.train_data.shape[1]\n",
    "\n",
    "        feature_extractor = [nn.Linear(input_dim, params['hidden_units_1']), \n",
    "                             nn.Dropout(p=params['dropout']),\n",
    "                             nn.ReLU()]\n",
    "        \n",
    "        self.feature_extractor = nn.Sequential(*feature_extractor) #L layer in diag\n",
    "\n",
    "        latent_basis =  [nn.Linear(params['hidden_units_1'], params['latent_space']),\n",
    "                         nn.Dropout(p=params['dropout']),\n",
    "                         nn.ReLU()]\n",
    "        \n",
    "        self.latent_basis = nn.Sequential(*latent_basis) #Z layer in diag\n",
    "        \n",
    "        #task-specific weights\n",
    "        self.MTL = nn.Linear(params['latent_space'], self.num_tasks) #S Layer in diag| also the prediction outputs\n",
    "        \n",
    "        #decoder weights\n",
    "        decTTF = [nn.Linear( self.num_tasks , params['latent_space']), nn.ReLU()] # A layer in diag or decoder task to feature layer \n",
    "        self.decTTF = nn.Sequential(*decTTF) \n",
    "\n",
    "        #uncertainty (aleatoric)\n",
    "        self.log_vars = torch.zeros(self.num_tasks, requires_grad=True, device=device)\n",
    "        \n",
    "        #discriminator\n",
    "        task_classifier = [nn.Linear(params[\"latent_space\"], params[\"n_units_disc\"]), \n",
    "                             nn.Dropout(p=params['dropout']), \n",
    "                             nn.ReLU(),\n",
    "                             nn.Linear(params[\"n_units_disc\"], 1), \n",
    "                             nn.Sigmoid()\n",
    "                            ]\n",
    "        self.task_classifier = nn.Sequential(*task_classifier)\n",
    "\n",
    "        self.prepare_data()\n",
    "\n",
    "    def forward(self, input_data, alpha):\n",
    "        x = self.feature_extractor(input_data) #usually the input data is treated as x\n",
    "        x = self.latent_basis(x) # here h \n",
    "        preds = self.MTL(x)\n",
    "        x_hat = self.decTTF(preds) # task transfer to features # here was h_hat\n",
    "        reverse_feature = ReverseLayerF.apply(x, alpha)\n",
    "        task_classifier_output = self.task_classifier(reverse_feature)\n",
    "        return preds, x, x_hat, task_classifier_output\n",
    "    \n",
    "    def prepare_data(self):\n",
    "        train_dataset = TensorDataset(torch.FloatTensor(self.train_data),\n",
    "                                      torch.FloatTensor(self.y_train))\n",
    "\n",
    "        \n",
    "        target_unl_dataset = TensorDataset(torch.FloatTensor(self.test_data))\n",
    "        \n",
    "        self.train_dataset = train_dataset\n",
    "        self.target_unl_dataset = target_unl_dataset\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        dataloader1 = DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True) #in vitro data\n",
    "        dataloader2 = DataLoader(self.target_unl_dataset, batch_size=self.bs_disc, shuffle=True) #in vivo data \n",
    "        self.len_dataloader = min(len(dataloader1), len(dataloader2))\n",
    "        x=zip(dataloader1, cycle(dataloader2))\n",
    "        return x\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        dataloader = DataLoader(self.target_unl_dataset, batch_size=len(self.target_unl_dataset), shuffle=False)\n",
    "        return dataloader\n",
    "    \n",
    "    def MSE_loss(self, x, x_hat):\n",
    "        mse_loss = torch.nn.MSELoss()\n",
    "        return mse_loss(x, x_hat)\n",
    "    \n",
    "    def task_mse_ignore_nan(self, preds, labels):\n",
    "        mse_loss = torch.nn.MSELoss(reduction='none')\n",
    "        per_task_loss = torch.zeros(labels.size(1), device=device)\n",
    "\n",
    "        for k in range(labels.size(1)):\n",
    "            precision = torch.exp(-self.log_vars[k]) #didnt understand this.\n",
    "            diff = mse_loss(preds[~torch.isnan(labels[:,k]), k], labels[~torch.isnan(labels[:,k]), k])\n",
    "            per_task_loss[k] = torch.mean(precision * diff + self.log_vars[k])\n",
    "\n",
    "        return torch.mean(per_task_loss[~torch.isnan(per_task_loss)]), per_task_loss\n",
    "\n",
    "    def mse_ignore_nan_test(self, preds, labels):\n",
    "        mse_loss = torch.nn.MSELoss(reduction='mean')\n",
    "        per_task_loss = torch.zeros(labels.size(1), device=device)\n",
    "        per_sample_loss = torch.zeros(labels.size(0), device=device)\n",
    "\n",
    "        \n",
    "        for k in range(labels.size(1)):\n",
    "            per_task_loss[k] = mse_loss(preds[~torch.isnan(labels[:,k]), k], labels[~torch.isnan(labels[:,k]), k])\n",
    "        \n",
    "        #per class loss\n",
    "        for k in range(labels.size(0)):\n",
    "            per_sample_loss[k] = mse_loss(preds[k, ~torch.isnan(labels[k,:])], labels[k, ~torch.isnan(labels[k, :])])\n",
    "        \n",
    "        \n",
    "        return torch.mean(per_task_loss[~torch.isnan(per_task_loss)]), per_task_loss,per_sample_loss\n",
    "    \n",
    "    def binary_classification_loss(self, preds, labels):\n",
    "        bin_loss = torch.nn.BCELoss()\n",
    "        return bin_loss(preds, labels)\n",
    "    \n",
    "    def train(self):\n",
    "\n",
    "        optimizer = torch.optim.Adagrad([\n",
    "            {'params': self.feature_extractor.parameters()},\n",
    "            {'params': self.latent_basis.parameters()},\n",
    "            {'params': self.MTL.parameters()},\n",
    "            {'params': self.decTTF.parameters()},\n",
    "            {'params': self.task_classifier.parameters()},\n",
    "            {'params': self.log_vars}\n",
    "        ], lr=self.learning_rate)\n",
    "        train_dataloader=self.train_dataloader()\n",
    "        for epoch in range(self.n_epoch):\n",
    "            for batch_idx,data in enumerate(train_dataloader):\n",
    "                x,y=data[0] #in vitro\n",
    "                x_unl=data[1][0] #invivo\n",
    "\n",
    "                p=float(batch_idx+epoch*self.len_dataloader)/self.n_epoch/self.len_dataloader\n",
    "                alpha=2. / (1. + np.exp(-10 * p)) - 1\n",
    "\n",
    "                preds_simulation=torch.zeros(y.size(0),y.size(1),self.passes,device=device) \n",
    "                for simulation in range(self.passes):\n",
    "                    preds, h, h_hat, domain_out_source = self.forward(x, alpha)\n",
    "                    preds_simulation[:,:, simulation]=preds\n",
    "                \n",
    "                preds_mean = torch.mean(preds_simulation, axis=2) #2D predictions after p passes\n",
    "                preds_var = torch.var(preds_simulation, axis=2) #2D\n",
    "                total_unc = torch.mean(preds_var, axis=0) # mu_t this is epistemic uncertainity 1D of tasks i.e y.shape[1]\n",
    "\n",
    "\n",
    "                m_loss, task_loss = self.task_mse_ignore_nan(preds_mean, y)\n",
    "                recon_loss= self.MSE_loss(h,h_hat)\n",
    "\n",
    "                a = 1 + (total_unc + torch.sum(torch.abs(self.decTTF[0].weight.T),1)) #multiplier to L_bnn\n",
    "\n",
    "                loss_weight = ( a[~torch.isnan(task_loss)] ) * task_loss[~torch.isnan(task_loss)]\n",
    "                loss_weight = torch.sum(loss_weight)\n",
    "\n",
    "                #Regularizer gamma(Z-sig(ZSA))\n",
    "                l1_S = self.mu * self.MTL.weight.norm(1)\n",
    "                L = self.latent_basis[0].weight.norm(2) + self.feature_extractor[0].weight.norm(2)\n",
    "                l2_L = self.lambda_ * L\n",
    "\n",
    "                #domain discriminator source\n",
    "                #normal data pass\n",
    "                # 0 is one class of invitro\n",
    "                zeros = torch.zeros(y.size(0), device=self.device)\n",
    "                d_loss_source = self.binary_classification_loss(domain_out_source, zeros)\n",
    "                \n",
    "                #domain discriminator target\n",
    "                #unlabelled data pass\n",
    "                #1 is second class which is invivo \n",
    "                preds, h, h_hat, domain_out_target = self.forward(x_unl, alpha)\n",
    "                ones = torch.ones(x_unl.size(0), device=self.device) \n",
    "                d_loss_target = self.binary_classification_loss(domain_out_target, ones)\n",
    "                \n",
    "                d_loss = d_loss_source + d_loss_target #l_adv loss or the adversarial loss \n",
    "\n",
    "                #total loss\n",
    "                loss = loss_weight + recon_loss + l1_S + l2_L + (self.lambda_disc *d_loss)\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                print({'total_loss': loss, 'source_loss': task_loss, 'disc_loss': d_loss})\n",
    "\n",
    "        return None #loss, m_loss, d_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4517c9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Tugda_da(net_params, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "320806c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c1fe63d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReverseLayerF(Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, alpha):\n",
    "        ctx.alpha = alpha\n",
    "\n",
    "        return x.view_as(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        output = grad_output.neg() * ctx.alpha\n",
    "\n",
    "        return output, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f39188b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0f5f7189",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "beeabbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu' if torch.backends.mps.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "be4958db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from itertools import cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ea6d58a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.autograd import Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0308ae7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu' if torch.backends.mps.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5f7928dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReverseLayerF(Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, alpha):\n",
    "        ctx.alpha = alpha\n",
    "\n",
    "        return x.view_as(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        output = grad_output.neg() * ctx.alpha\n",
    "\n",
    "        return output, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8e7b68b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tugda_da(nn.Module):\n",
    "\n",
    "    def __init__(self,params,train_data,test_data,y_train,y_test):\n",
    "        super(Tugda_da,self).__init__()\n",
    "        self.learning_rate = params['lr']\n",
    "        self.batch_size = params['bs_source']\n",
    "        self.mu = params['mu']\n",
    "        self.lambda_ = params['lambda_']\n",
    "        self.gamma = params['gamma']\n",
    "        self.lambda_disc = params['lambda_disc']\n",
    "        self.bs_disc = params['bs_disc']\n",
    "        self.n_epoch = params['n_epochs']\n",
    "        self.passes = params['passes']\n",
    "        self.num_tasks = params['num_tasks']\n",
    "\n",
    "        self.train_data = train_data\n",
    "        self.y_train = y_train\n",
    "        self.test_data = test_data\n",
    "        self.y_test = y_test\n",
    "\n",
    "        input_dim=self.train_data.shape[1]\n",
    "\n",
    "        feature_extractor = [nn.Linear(input_dim, params['hidden_units_1']), \n",
    "                             nn.Dropout(p=params['dropout']),\n",
    "                             nn.ReLU()]\n",
    "        \n",
    "        self.feature_extractor = nn.Sequential(*feature_extractor) #L layer in diag\n",
    "\n",
    "        latent_basis =  [nn.Linear(params['hidden_units_1'], params['latent_space']),\n",
    "                         nn.Dropout(p=params['dropout']),\n",
    "                         nn.ReLU()]\n",
    "        \n",
    "        self.latent_basis = nn.Sequential(*latent_basis) #Z layer in diag\n",
    "        \n",
    "        #task-specific weights\n",
    "        self.MTL = nn.Linear(params['latent_space'], self.num_tasks) #S Layer in diag| also the prediction outputs\n",
    "        \n",
    "        #decoder weights\n",
    "        decTTF = [nn.Linear( self.num_tasks , params['latent_space']), nn.ReLU()] # A layer in diag or decoder task to feature layer \n",
    "        self.decTTF = nn.Sequential(*decTTF) \n",
    "\n",
    "        #uncertainty (aleatoric)\n",
    "        self.log_vars = torch.zeros(self.num_tasks, requires_grad=True, device=device)\n",
    "        \n",
    "        #discriminator\n",
    "        task_classifier = [nn.Linear(params[\"latent_space\"], params[\"n_units_disc\"]), \n",
    "                             nn.Dropout(p=params['dropout']), \n",
    "                             nn.ReLU(),\n",
    "                             nn.Linear(params[\"n_units_disc\"], 1), \n",
    "                             nn.Sigmoid()\n",
    "                            ]\n",
    "        self.task_classifier = nn.Sequential(*task_classifier)\n",
    "\n",
    "        self.prepare_data()\n",
    "\n",
    "    def forward(self, input_data, alpha):\n",
    "        x = self.feature_extractor(input_data) #usually the input data is treated as x\n",
    "        x = self.latent_basis(x) # here h \n",
    "        preds = self.MTL(x)\n",
    "        x_hat = self.decTTF(preds) # task transfer to features # here was h_hat\n",
    "        reverse_feature = ReverseLayerF.apply(x, alpha)\n",
    "        task_classifier_output = self.task_classifier(reverse_feature)\n",
    "        return preds, x, x_hat, task_classifier_output\n",
    "    \n",
    "    def prepare_data(self):\n",
    "        train_dataset = TensorDataset(torch.FloatTensor(self.train_data),\n",
    "                                      torch.FloatTensor(self.y_train))\n",
    "\n",
    "        \n",
    "        target_unl_dataset = TensorDataset(torch.FloatTensor(self.test_data))\n",
    "        \n",
    "        self.train_dataset = train_dataset\n",
    "        self.target_unl_dataset = target_unl_dataset\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        dataloader1 = DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True) #in vitro data\n",
    "        dataloader2 = DataLoader(self.target_unl_dataset, batch_size=self.bs_disc, shuffle=True) #in vivo data \n",
    "        self.len_dataloader = min(len(dataloader1), len(dataloader2))\n",
    "        x=zip(dataloader1, cycle(dataloader2))\n",
    "        return x\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        dataloader = DataLoader(self.target_unl_dataset, batch_size=len(self.target_unl_dataset), shuffle=False)\n",
    "        return dataloader\n",
    "    \n",
    "    def MSE_loss(self, x, x_hat):\n",
    "        mse_loss = torch.nn.MSELoss()\n",
    "        return mse_loss(x, x_hat)\n",
    "    \n",
    "    def task_mse_ignore_nan(self, preds, labels):\n",
    "        mse_loss = torch.nn.MSELoss(reduction='none')\n",
    "        per_task_loss = torch.zeros(labels.size(1), device=device)\n",
    "\n",
    "        for k in range(labels.size(1)):\n",
    "            precision = torch.exp(-self.log_vars[k]) #didnt understand this.\n",
    "            diff = mse_loss(preds[~torch.isnan(labels[:,k]), k], labels[~torch.isnan(labels[:,k]), k])\n",
    "            per_task_loss[k] = torch.mean(precision * diff + self.log_vars[k])\n",
    "\n",
    "        return torch.mean(per_task_loss[~torch.isnan(per_task_loss)]), per_task_loss\n",
    "\n",
    "    def mse_ignore_nan_test(self, preds, labels):\n",
    "        mse_loss = torch.nn.MSELoss(reduction='mean')\n",
    "        per_task_loss = torch.zeros(labels.size(1), device=device)\n",
    "        per_sample_loss = torch.zeros(labels.size(0), device=device)\n",
    "\n",
    "        \n",
    "        for k in range(labels.size(1)):\n",
    "            per_task_loss[k] = mse_loss(preds[~torch.isnan(labels[:,k]), k], labels[~torch.isnan(labels[:,k]), k])\n",
    "        \n",
    "        #per class loss\n",
    "        for k in range(labels.size(0)):\n",
    "            per_sample_loss[k] = mse_loss(preds[k, ~torch.isnan(labels[k,:])], labels[k, ~torch.isnan(labels[k, :])])\n",
    "        \n",
    "        \n",
    "        return torch.mean(per_task_loss[~torch.isnan(per_task_loss)]), per_task_loss,per_sample_loss\n",
    "    \n",
    "    def binary_classification_loss(self, preds, labels):\n",
    "        bin_loss = torch.nn.BCELoss()\n",
    "        return bin_loss(preds, labels)\n",
    "    \n",
    "    def train(self):\n",
    "\n",
    "        optimizer = torch.optim.Adagrad([\n",
    "            {'params': self.feature_extractor.parameters()},\n",
    "            {'params': self.latent_basis.parameters()},\n",
    "            {'params': self.MTL.parameters()},\n",
    "            {'params': self.decTTF.parameters()},\n",
    "            {'params': self.task_classifier.parameters()},\n",
    "            {'params': self.log_vars}\n",
    "        ], lr=self.learning_rate)\n",
    "        train_dataloader=self.train_dataloader()\n",
    "        for epoch in range(self.n_epoch):\n",
    "            for batch_idx,data in enumerate(train_dataloader):\n",
    "                x,y=data[0] #in vitro\n",
    "                x_unl=data[1][0] #invivo\n",
    "\n",
    "                p=float(batch_idx+epoch*self.len_dataloader)/self.n_epoch/self.len_dataloader\n",
    "                alpha=2. / (1. + np.exp(-10 * p)) - 1\n",
    "\n",
    "                preds_simulation=torch.zeros(y.size(0),y.size(1),self.passes,device=device) \n",
    "                for simulation in range(self.passes):\n",
    "                    preds, h, h_hat, domain_out_source = self.forward(x, alpha)\n",
    "                    preds_simulation[:,:, simulation]=preds\n",
    "                \n",
    "                preds_mean = torch.mean(preds_simulation, axis=2) #2D predictions after p passes\n",
    "                preds_var = torch.var(preds_simulation, axis=2) #2D\n",
    "                total_unc = torch.mean(preds_var, axis=0) # mu_t this is epistemic uncertainity 1D of tasks i.e y.shape[1]\n",
    "\n",
    "\n",
    "                m_loss, task_loss = self.task_mse_ignore_nan(preds_mean, y)\n",
    "                recon_loss= self.MSE_loss(h,h_hat)\n",
    "\n",
    "                a = 1 + (total_unc + torch.sum(torch.abs(self.decTTF[0].weight.T),1)) #multiplier to L_bnn\n",
    "\n",
    "                loss_weight = ( a[~torch.isnan(task_loss)] ) * task_loss[~torch.isnan(task_loss)]\n",
    "                loss_weight = torch.sum(loss_weight)\n",
    "\n",
    "                #Regularizer gamma(Z-sig(ZSA))\n",
    "                l1_S = self.mu * self.MTL.weight.norm(1)\n",
    "                L = self.latent_basis[0].weight.norm(2) + self.feature_extractor[0].weight.norm(2)\n",
    "                l2_L = self.lambda_ * L\n",
    "\n",
    "                #domain discriminator source\n",
    "                #normal data pass\n",
    "                # 0 is one class of invitro\n",
    "                zeros = torch.zeros(y.size(0), device=self.device)\n",
    "                d_loss_source = self.binary_classification_loss(domain_out_source, zeros)\n",
    "                \n",
    "                #domain discriminator target\n",
    "                #unlabelled data pass\n",
    "                #1 is second class which is invivo \n",
    "                preds, h, h_hat, domain_out_target = self.forward(x_unl, alpha)\n",
    "                ones = torch.ones(x_unl.size(0), device=self.device) \n",
    "                d_loss_target = self.binary_classification_loss(domain_out_target, ones)\n",
    "                \n",
    "                d_loss = d_loss_source + d_loss_target #l_adv loss or the adversarial loss \n",
    "\n",
    "                #total loss\n",
    "                loss = loss_weight + recon_loss + l1_S + l2_L + (self.lambda_disc *d_loss)\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                print({'total_loss': loss, 'source_loss': task_loss, 'disc_loss': d_loss})\n",
    "\n",
    "        return None #loss, m_loss, d_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "83689c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cell-line dataset;\n",
    "gdsc_dataset = pd.read_csv('data/GDSCDA_fpkm_AUC_all_drugs.zip', index_col=0)\n",
    "#gene set range\n",
    "gene_list = gdsc_dataset.columns[0:1780]\n",
    "drug_list = gdsc_dataset.columns[1780:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0188a8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pdx novartis dataset;\n",
    "pdx_dataset = pd.read_csv('data/PDX_MTL_DA.csv', index_col=0)\n",
    "drugs_pdx = pdx_dataset.columns[1780:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "94bc7acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_params = {\n",
    " 'hidden_units_1': 1500,\n",
    " 'latent_space': 800,\n",
    " 'lr': 0.001,\n",
    " 'dropout': 0.1,\n",
    " 'mu': 1,\n",
    " 'lambda_': 1,\n",
    " 'gamma': 0.01,\n",
    " 'n_units_disc': 500,\n",
    " 'n_epochs': 5, #50\n",
    " 'bs_disc': 64,\n",
    " 'bs_source': 300,\n",
    " 'lambda_disc': 0.3,\n",
    " 'num_tasks': 200,\n",
    " 'passes': 10}#50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "09453a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_list = gdsc_dataset.columns[0:1780]\n",
    "drug_list = gdsc_dataset.columns[1780:]\n",
    "\n",
    "X_train = gdsc_dataset[gene_list].values\n",
    "y_train = gdsc_dataset[drug_list].values\n",
    "\n",
    "X_test = pdx_dataset[gene_list].values\n",
    "y_test = pdx_dataset[drugs_pdx].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "705cdf53",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Tugda_da(net_params, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f44541f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "dd7bca15",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Tugda_da(net_params, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6c8efc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "45270615",
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_list = gdsc_dataset.columns[0:1780]\n",
    "drug_list = gdsc_dataset.columns[1780:]\n",
    "\n",
    "X_train = gdsc_dataset[gene_list].values\n",
    "y_train = gdsc_dataset[drug_list].values\n",
    "\n",
    "X_test = pdx_dataset[gene_list].values\n",
    "y_test = pdx_dataset[drugs_pdx].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "730b7af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Tugda_da(net_params, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f69b844d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2eb3cb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tugda_da(nn.Module):\n",
    "\n",
    "    def __init__(self,params,train_data,test_data,y_train,y_test):\n",
    "        super(Tugda_da,self).__init__()\n",
    "        self.learning_rate = params['lr']\n",
    "        self.batch_size = params['bs_source']\n",
    "        self.mu = params['mu']\n",
    "        self.lambda_ = params['lambda_']\n",
    "        self.gamma = params['gamma']\n",
    "        self.lambda_disc = params['lambda_disc']\n",
    "        self.bs_disc = params['bs_disc']\n",
    "        self.n_epoch = params['n_epochs']\n",
    "        self.passes = params['passes']\n",
    "        self.num_tasks = params['num_tasks']\n",
    "\n",
    "        self.train_data = train_data\n",
    "        self.y_train = y_train\n",
    "        self.test_data = test_data\n",
    "        self.y_test = y_test\n",
    "\n",
    "        input_dim=self.train_data.shape[1]\n",
    "\n",
    "        feature_extractor = [nn.Linear(input_dim, params['hidden_units_1']), \n",
    "                             nn.Dropout(p=params['dropout']),\n",
    "                             nn.ReLU()]\n",
    "        \n",
    "        self.feature_extractor = nn.Sequential(*feature_extractor) #L layer in diag\n",
    "\n",
    "        latent_basis =  [nn.Linear(params['hidden_units_1'], params['latent_space']),\n",
    "                         nn.Dropout(p=params['dropout']),\n",
    "                         nn.ReLU()]\n",
    "        \n",
    "        self.latent_basis = nn.Sequential(*latent_basis) #Z layer in diag\n",
    "        \n",
    "        #task-specific weights\n",
    "        self.MTL = nn.Linear(params['latent_space'], self.num_tasks) #S Layer in diag| also the prediction outputs\n",
    "        \n",
    "        #decoder weights\n",
    "        decTTF = [nn.Linear( self.num_tasks , params['latent_space']), nn.ReLU()] # A layer in diag or decoder task to feature layer \n",
    "        self.decTTF = nn.Sequential(*decTTF) \n",
    "\n",
    "        #uncertainty (aleatoric)\n",
    "        self.log_vars = torch.zeros(self.num_tasks, requires_grad=True, device=device)\n",
    "        \n",
    "        #discriminator\n",
    "        task_classifier = [nn.Linear(params[\"latent_space\"], params[\"n_units_disc\"]), \n",
    "                             nn.Dropout(p=params['dropout']), \n",
    "                             nn.ReLU(),\n",
    "                             nn.Linear(params[\"n_units_disc\"], 1), \n",
    "                             nn.Sigmoid()\n",
    "                            ]\n",
    "        self.task_classifier = nn.Sequential(*task_classifier)\n",
    "\n",
    "        self.prepare_data()\n",
    "\n",
    "    def forward(self, input_data, alpha):\n",
    "        x = self.feature_extractor(input_data) #usually the input data is treated as x\n",
    "        x = self.latent_basis(x) # here h \n",
    "        preds = self.MTL(x)\n",
    "        x_hat = self.decTTF(preds) # task transfer to features # here was h_hat\n",
    "        reverse_feature = ReverseLayerF.apply(x, alpha)\n",
    "        task_classifier_output = self.task_classifier(reverse_feature)\n",
    "        return preds, x, x_hat, task_classifier_output\n",
    "    \n",
    "    def prepare_data(self):\n",
    "        train_dataset = TensorDataset(torch.FloatTensor(self.train_data),\n",
    "                                      torch.FloatTensor(self.y_train))\n",
    "\n",
    "        \n",
    "        target_unl_dataset = TensorDataset(torch.FloatTensor(self.test_data))\n",
    "        \n",
    "        self.train_dataset = train_dataset\n",
    "        self.target_unl_dataset = target_unl_dataset\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        dataloader1 = DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True) #in vitro data\n",
    "        dataloader2 = DataLoader(self.target_unl_dataset, batch_size=self.bs_disc, shuffle=True) #in vivo data \n",
    "        self.len_dataloader = min(len(dataloader1), len(dataloader2))\n",
    "        x=zip(dataloader1, cycle(dataloader2))\n",
    "        return x\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        dataloader = DataLoader(self.target_unl_dataset, batch_size=len(self.target_unl_dataset), shuffle=False)\n",
    "        return dataloader\n",
    "    \n",
    "    def MSE_loss(self, x, x_hat):\n",
    "        mse_loss = torch.nn.MSELoss()\n",
    "        return mse_loss(x, x_hat)\n",
    "    \n",
    "    def task_mse_ignore_nan(self, preds, labels):\n",
    "        mse_loss = torch.nn.MSELoss(reduction='none')\n",
    "        per_task_loss = torch.zeros(labels.size(1), device=device)\n",
    "\n",
    "        for k in range(labels.size(1)):\n",
    "            precision = torch.exp(-self.log_vars[k]) #didnt understand this.\n",
    "            diff = mse_loss(preds[~torch.isnan(labels[:,k]), k], labels[~torch.isnan(labels[:,k]), k])\n",
    "            per_task_loss[k] = torch.mean(precision * diff + self.log_vars[k])\n",
    "\n",
    "        return torch.mean(per_task_loss[~torch.isnan(per_task_loss)]), per_task_loss\n",
    "\n",
    "    def mse_ignore_nan_test(self, preds, labels):\n",
    "        mse_loss = torch.nn.MSELoss(reduction='mean')\n",
    "        per_task_loss = torch.zeros(labels.size(1), device=device)\n",
    "        per_sample_loss = torch.zeros(labels.size(0), device=device)\n",
    "\n",
    "        \n",
    "        for k in range(labels.size(1)):\n",
    "            per_task_loss[k] = mse_loss(preds[~torch.isnan(labels[:,k]), k], labels[~torch.isnan(labels[:,k]), k])\n",
    "        \n",
    "        #per class loss\n",
    "        for k in range(labels.size(0)):\n",
    "            per_sample_loss[k] = mse_loss(preds[k, ~torch.isnan(labels[k,:])], labels[k, ~torch.isnan(labels[k, :])])\n",
    "        \n",
    "        \n",
    "        return torch.mean(per_task_loss[~torch.isnan(per_task_loss)]), per_task_loss,per_sample_loss\n",
    "    \n",
    "    def binary_classification_loss(self, preds, labels):\n",
    "        bin_loss = torch.nn.BCELoss()\n",
    "        return bin_loss(preds, labels)\n",
    "    \n",
    "    def train(self):\n",
    "\n",
    "        optimizer = torch.optim.Adagrad([\n",
    "            {'params': self.feature_extractor.parameters()},\n",
    "            {'params': self.latent_basis.parameters()},\n",
    "            {'params': self.MTL.parameters()},\n",
    "            {'params': self.decTTF.parameters()},\n",
    "            {'params': self.task_classifier.parameters()},\n",
    "            {'params': self.log_vars}\n",
    "        ], lr=self.learning_rate)\n",
    "        train_dataloader=self.train_dataloader()\n",
    "        for epoch in range(self.n_epoch):\n",
    "            for batch_idx,data in enumerate(train_dataloader):\n",
    "                x,y=data[0] #in vitro\n",
    "                x_unl=data[1][0] #invivo\n",
    "\n",
    "                p=float(batch_idx+epoch*self.len_dataloader)/self.n_epoch/self.len_dataloader\n",
    "                alpha=2. / (1. + np.exp(-10 * p)) - 1\n",
    "\n",
    "                preds_simulation=torch.zeros(y.size(0),y.size(1),self.passes)#,device=device) \n",
    "                for simulation in range(self.passes):\n",
    "                    preds, h, h_hat, domain_out_source = self.forward(x, alpha)\n",
    "                    preds_simulation[:,:, simulation]=preds\n",
    "                \n",
    "                preds_mean = torch.mean(preds_simulation, axis=2) #2D predictions after p passes\n",
    "                preds_var = torch.var(preds_simulation, axis=2) #2D\n",
    "                total_unc = torch.mean(preds_var, axis=0) # mu_t this is epistemic uncertainity 1D of tasks i.e y.shape[1]\n",
    "\n",
    "\n",
    "                m_loss, task_loss = self.task_mse_ignore_nan(preds_mean, y)\n",
    "                recon_loss= self.MSE_loss(h,h_hat)\n",
    "\n",
    "                a = 1 + (total_unc + torch.sum(torch.abs(self.decTTF[0].weight.T),1)) #multiplier to L_bnn\n",
    "\n",
    "                loss_weight = ( a[~torch.isnan(task_loss)] ) * task_loss[~torch.isnan(task_loss)]\n",
    "                loss_weight = torch.sum(loss_weight)\n",
    "\n",
    "                #Regularizer gamma(Z-sig(ZSA))\n",
    "                l1_S = self.mu * self.MTL.weight.norm(1)\n",
    "                L = self.latent_basis[0].weight.norm(2) + self.feature_extractor[0].weight.norm(2)\n",
    "                l2_L = self.lambda_ * L\n",
    "\n",
    "                #domain discriminator source\n",
    "                #normal data pass\n",
    "                # 0 is one class of invitro\n",
    "                zeros = torch.zeros(y.size(0))#, device=self.device)\n",
    "                d_loss_source = self.binary_classification_loss(domain_out_source, zeros)\n",
    "                \n",
    "                #domain discriminator target\n",
    "                #unlabelled data pass\n",
    "                #1 is second class which is invivo \n",
    "                preds, h, h_hat, domain_out_target = self.forward(x_unl, alpha)\n",
    "                ones = torch.ones(x_unl.size(0))#, device=self.device) \n",
    "                d_loss_target = self.binary_classification_loss(domain_out_target, ones)\n",
    "                \n",
    "                d_loss = d_loss_source + d_loss_target #l_adv loss or the adversarial loss \n",
    "\n",
    "                #total loss\n",
    "                loss = loss_weight + recon_loss + l1_S + l2_L + (self.lambda_disc *d_loss)\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                print({'total_loss': loss, 'source_loss': task_loss, 'disc_loss': d_loss})\n",
    "\n",
    "        return None #loss, m_loss, d_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0bd95e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Tugda_da(net_params, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0abd3084",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "47ca66d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tugda_da(nn.Module):\n",
    "\n",
    "    def __init__(self,params,train_data,test_data,y_train,y_test):\n",
    "        super(Tugda_da,self).__init__()\n",
    "        self.learning_rate = params['lr']\n",
    "        self.batch_size = params['bs_source']\n",
    "        self.mu = params['mu']\n",
    "        self.lambda_ = params['lambda_']\n",
    "        self.gamma = params['gamma']\n",
    "        self.lambda_disc = params['lambda_disc']\n",
    "        self.bs_disc = params['bs_disc']\n",
    "        self.n_epoch = params['n_epochs']\n",
    "        self.passes = params['passes']\n",
    "        self.num_tasks = params['num_tasks']\n",
    "\n",
    "        self.train_data = train_data\n",
    "        self.y_train = y_train\n",
    "        self.test_data = test_data\n",
    "        self.y_test = y_test\n",
    "\n",
    "        input_dim=self.train_data.shape[1]\n",
    "\n",
    "        feature_extractor = [nn.Linear(input_dim, params['hidden_units_1']), \n",
    "                             nn.Dropout(p=params['dropout']),\n",
    "                             nn.ReLU()]\n",
    "        \n",
    "        self.feature_extractor = nn.Sequential(*feature_extractor) #L layer in diag\n",
    "\n",
    "        latent_basis =  [nn.Linear(params['hidden_units_1'], params['latent_space']),\n",
    "                         nn.Dropout(p=params['dropout']),\n",
    "                         nn.ReLU()]\n",
    "        \n",
    "        self.latent_basis = nn.Sequential(*latent_basis) #Z layer in diag\n",
    "        \n",
    "        #task-specific weights\n",
    "        self.MTL = nn.Linear(params['latent_space'], self.num_tasks) #S Layer in diag| also the prediction outputs\n",
    "        \n",
    "        #decoder weights\n",
    "        decTTF = [nn.Linear( self.num_tasks , params['latent_space']), nn.ReLU()] # A layer in diag or decoder task to feature layer \n",
    "        self.decTTF = nn.Sequential(*decTTF) \n",
    "\n",
    "        #uncertainty (aleatoric)\n",
    "        self.log_vars = torch.zeros(self.num_tasks, requires_grad=True, device=device)\n",
    "        \n",
    "        #discriminator\n",
    "        task_classifier = [nn.Linear(params[\"latent_space\"], params[\"n_units_disc\"]), \n",
    "                             nn.Dropout(p=params['dropout']), \n",
    "                             nn.ReLU(),\n",
    "                             nn.Linear(params[\"n_units_disc\"], 1), \n",
    "                             nn.Sigmoid()\n",
    "                            ]\n",
    "        self.task_classifier = nn.Sequential(*task_classifier)\n",
    "\n",
    "        self.prepare_data()\n",
    "\n",
    "    def forward(self, input_data, alpha):\n",
    "        x = self.feature_extractor(input_data) #usually the input data is treated as x\n",
    "        x = self.latent_basis(x) # here h \n",
    "        preds = self.MTL(x)\n",
    "        x_hat = self.decTTF(preds) # task transfer to features # here was h_hat\n",
    "        reverse_feature = ReverseLayerF.apply(x, alpha)\n",
    "        task_classifier_output = self.task_classifier(reverse_feature)\n",
    "        return preds, x, x_hat, task_classifier_output\n",
    "    \n",
    "    def prepare_data(self):\n",
    "        train_dataset = TensorDataset(torch.FloatTensor(self.train_data),\n",
    "                                      torch.FloatTensor(self.y_train))\n",
    "\n",
    "        \n",
    "        target_unl_dataset = TensorDataset(torch.FloatTensor(self.test_data))\n",
    "        \n",
    "        self.train_dataset = train_dataset\n",
    "        self.target_unl_dataset = target_unl_dataset\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        dataloader1 = DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True) #in vitro data\n",
    "        dataloader2 = DataLoader(self.target_unl_dataset, batch_size=self.bs_disc, shuffle=True) #in vivo data \n",
    "        self.len_dataloader = min(len(dataloader1), len(dataloader2))\n",
    "        x=zip(dataloader1, cycle(dataloader2))\n",
    "        return x\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        dataloader = DataLoader(self.target_unl_dataset, batch_size=len(self.target_unl_dataset), shuffle=False)\n",
    "        return dataloader\n",
    "    \n",
    "    def MSE_loss(self, x, x_hat):\n",
    "        mse_loss = torch.nn.MSELoss()\n",
    "        return mse_loss(x, x_hat)\n",
    "    \n",
    "    def task_mse_ignore_nan(self, preds, labels):\n",
    "        mse_loss = torch.nn.MSELoss(reduction='none')\n",
    "        per_task_loss = torch.zeros(labels.size(1), device=device)\n",
    "\n",
    "        for k in range(labels.size(1)):\n",
    "            precision = torch.exp(-self.log_vars[k]) #didnt understand this.\n",
    "            diff = mse_loss(preds[~torch.isnan(labels[:,k]), k], labels[~torch.isnan(labels[:,k]), k])\n",
    "            per_task_loss[k] = torch.mean(precision * diff + self.log_vars[k])\n",
    "\n",
    "        return torch.mean(per_task_loss[~torch.isnan(per_task_loss)]), per_task_loss\n",
    "\n",
    "    def mse_ignore_nan_test(self, preds, labels):\n",
    "        mse_loss = torch.nn.MSELoss(reduction='mean')\n",
    "        per_task_loss = torch.zeros(labels.size(1), device=device)\n",
    "        per_sample_loss = torch.zeros(labels.size(0), device=device)\n",
    "\n",
    "        \n",
    "        for k in range(labels.size(1)):\n",
    "            per_task_loss[k] = mse_loss(preds[~torch.isnan(labels[:,k]), k], labels[~torch.isnan(labels[:,k]), k])\n",
    "        \n",
    "        #per class loss\n",
    "        for k in range(labels.size(0)):\n",
    "            per_sample_loss[k] = mse_loss(preds[k, ~torch.isnan(labels[k,:])], labels[k, ~torch.isnan(labels[k, :])])\n",
    "        \n",
    "        \n",
    "        return torch.mean(per_task_loss[~torch.isnan(per_task_loss)]), per_task_loss,per_sample_loss\n",
    "    \n",
    "    def binary_classification_loss(self, preds, labels):\n",
    "        bin_loss = torch.nn.BCELoss()\n",
    "        return bin_loss(preds, labels)\n",
    "    \n",
    "    def train(self):\n",
    "\n",
    "        optimizer = torch.optim.Adagrad([\n",
    "            {'params': self.feature_extractor.parameters()},\n",
    "            {'params': self.latent_basis.parameters()},\n",
    "            {'params': self.MTL.parameters()},\n",
    "            {'params': self.decTTF.parameters()},\n",
    "            {'params': self.task_classifier.parameters()},\n",
    "            {'params': self.log_vars}\n",
    "        ], lr=self.learning_rate)\n",
    "        train_dataloader=self.train_dataloader()\n",
    "        for epoch in range(self.n_epoch):\n",
    "            for batch_idx,data in enumerate(train_dataloader):\n",
    "                x,y=data[0] #in vitro\n",
    "                x_unl=data[1][0] #invivo\n",
    "\n",
    "                p=float(batch_idx+epoch*self.len_dataloader)/self.n_epoch/self.len_dataloader\n",
    "                alpha=2. / (1. + np.exp(-10 * p)) - 1\n",
    "\n",
    "                preds_simulation=torch.zeros(y.size(0),y.size(1),self.passes)#,device=device) \n",
    "                for simulation in range(self.passes):\n",
    "                    preds, h, h_hat, domain_out_source = self.forward(x, alpha)\n",
    "                    preds_simulation[:,:, simulation]=preds\n",
    "                \n",
    "                preds_mean = torch.mean(preds_simulation, axis=2) #2D predictions after p passes\n",
    "                preds_var = torch.var(preds_simulation, axis=2) #2D\n",
    "                total_unc = torch.mean(preds_var, axis=0) # mu_t this is epistemic uncertainity 1D of tasks i.e y.shape[1]\n",
    "\n",
    "\n",
    "                m_loss, task_loss = self.task_mse_ignore_nan(preds_mean, y)\n",
    "                recon_loss= self.MSE_loss(h,h_hat)\n",
    "\n",
    "                a = 1 + (total_unc + torch.sum(torch.abs(self.decTTF[0].weight.T),1)) #multiplier to L_bnn\n",
    "\n",
    "                loss_weight = ( a[~torch.isnan(task_loss)] ) * task_loss[~torch.isnan(task_loss)]\n",
    "                loss_weight = torch.sum(loss_weight)\n",
    "\n",
    "                #Regularizer gamma(Z-sig(ZSA))\n",
    "                l1_S = self.mu * self.MTL.weight.norm(1)\n",
    "                L = self.latent_basis[0].weight.norm(2) + self.feature_extractor[0].weight.norm(2)\n",
    "                l2_L = self.lambda_ * L\n",
    "\n",
    "                #domain discriminator source\n",
    "                #normal data pass\n",
    "                # 0 is one class of invitro\n",
    "                zeros = torch.zeros(y.size(0),1)#, device=self.device)\n",
    "                d_loss_source = self.binary_classification_loss(domain_out_source, zeros)\n",
    "                \n",
    "                #domain discriminator target\n",
    "                #unlabelled data pass\n",
    "                #1 is second class which is invivo \n",
    "                preds, h, h_hat, domain_out_target = self.forward(x_unl, alpha)\n",
    "                ones = torch.ones(x_unl.size(0),1)#, device=self.device) \n",
    "                d_loss_target = self.binary_classification_loss(domain_out_target, ones)\n",
    "                \n",
    "                d_loss = d_loss_source + d_loss_target #l_adv loss or the adversarial loss \n",
    "\n",
    "                #total loss\n",
    "                loss = loss_weight + recon_loss + l1_S + l2_L + (self.lambda_disc *d_loss)\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                print({'total_loss': loss, 'source_loss': task_loss, 'disc_loss': d_loss})\n",
    "\n",
    "        return None #loss, m_loss, d_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7f52bbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c5a90d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tugda_da(nn.Module):\n",
    "\n",
    "    def __init__(self,params,train_data,test_data,y_train,y_test):\n",
    "        super(Tugda_da,self).__init__()\n",
    "        self.learning_rate = params['lr']\n",
    "        self.batch_size = params['bs_source']\n",
    "        self.mu = params['mu']\n",
    "        self.lambda_ = params['lambda_']\n",
    "        self.gamma = params['gamma']\n",
    "        self.lambda_disc = params['lambda_disc']\n",
    "        self.bs_disc = params['bs_disc']\n",
    "        self.n_epoch = params['n_epochs']\n",
    "        self.passes = params['passes']\n",
    "        self.num_tasks = params['num_tasks']\n",
    "\n",
    "        self.train_data = train_data\n",
    "        self.y_train = y_train\n",
    "        self.test_data = test_data\n",
    "        self.y_test = y_test\n",
    "\n",
    "        input_dim=self.train_data.shape[1]\n",
    "\n",
    "        feature_extractor = [nn.Linear(input_dim, params['hidden_units_1']), \n",
    "                             nn.Dropout(p=params['dropout']),\n",
    "                             nn.ReLU()]\n",
    "        \n",
    "        self.feature_extractor = nn.Sequential(*feature_extractor) #L layer in diag\n",
    "\n",
    "        latent_basis =  [nn.Linear(params['hidden_units_1'], params['latent_space']),\n",
    "                         nn.Dropout(p=params['dropout']),\n",
    "                         nn.ReLU()]\n",
    "        \n",
    "        self.latent_basis = nn.Sequential(*latent_basis) #Z layer in diag\n",
    "        \n",
    "        #task-specific weights\n",
    "        self.MTL = nn.Linear(params['latent_space'], self.num_tasks) #S Layer in diag| also the prediction outputs\n",
    "        \n",
    "        #decoder weights\n",
    "        decTTF = [nn.Linear( self.num_tasks , params['latent_space']), nn.ReLU()] # A layer in diag or decoder task to feature layer \n",
    "        self.decTTF = nn.Sequential(*decTTF) \n",
    "\n",
    "        #uncertainty (aleatoric)\n",
    "        self.log_vars = torch.zeros(self.num_tasks, requires_grad=True, device=device)\n",
    "        \n",
    "        #discriminator\n",
    "        task_classifier = [nn.Linear(params[\"latent_space\"], params[\"n_units_disc\"]), \n",
    "                             nn.Dropout(p=params['dropout']), \n",
    "                             nn.ReLU(),\n",
    "                             nn.Linear(params[\"n_units_disc\"], 1), \n",
    "                             nn.Sigmoid()\n",
    "                            ]\n",
    "        self.task_classifier = nn.Sequential(*task_classifier)\n",
    "\n",
    "        self.prepare_data()\n",
    "\n",
    "    def forward(self, input_data, alpha):\n",
    "        x = self.feature_extractor(input_data) #usually the input data is treated as x\n",
    "        x = self.latent_basis(x) # here h \n",
    "        preds = self.MTL(x)\n",
    "        x_hat = self.decTTF(preds) # task transfer to features # here was h_hat\n",
    "        reverse_feature = ReverseLayerF.apply(x, alpha)\n",
    "        task_classifier_output = self.task_classifier(reverse_feature)\n",
    "        return preds, x, x_hat, task_classifier_output\n",
    "    \n",
    "    def prepare_data(self):\n",
    "        train_dataset = TensorDataset(torch.FloatTensor(self.train_data),\n",
    "                                      torch.FloatTensor(self.y_train))\n",
    "\n",
    "        \n",
    "        target_unl_dataset = TensorDataset(torch.FloatTensor(self.test_data))\n",
    "        \n",
    "        self.train_dataset = train_dataset\n",
    "        self.target_unl_dataset = target_unl_dataset\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        dataloader1 = DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True) #in vitro data\n",
    "        dataloader2 = DataLoader(self.target_unl_dataset, batch_size=self.bs_disc, shuffle=True) #in vivo data \n",
    "        self.len_dataloader = min(len(dataloader1), len(dataloader2))\n",
    "        x=zip(dataloader1, cycle(dataloader2))\n",
    "        return x\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        dataloader = DataLoader(self.target_unl_dataset, batch_size=len(self.target_unl_dataset), shuffle=False)\n",
    "        return dataloader\n",
    "    \n",
    "    def MSE_loss(self, x, x_hat):\n",
    "        mse_loss = torch.nn.MSELoss()\n",
    "        return mse_loss(x, x_hat)\n",
    "    \n",
    "    def task_mse_ignore_nan(self, preds, labels):\n",
    "        mse_loss = torch.nn.MSELoss(reduction='none')\n",
    "        per_task_loss = torch.zeros(labels.size(1), device=device)\n",
    "\n",
    "        for k in range(labels.size(1)):\n",
    "            precision = torch.exp(-self.log_vars[k]) #didnt understand this.\n",
    "            diff = mse_loss(preds[~torch.isnan(labels[:,k]), k], labels[~torch.isnan(labels[:,k]), k])\n",
    "            per_task_loss[k] = torch.mean(precision * diff + self.log_vars[k])\n",
    "\n",
    "        return torch.mean(per_task_loss[~torch.isnan(per_task_loss)]), per_task_loss\n",
    "\n",
    "    def mse_ignore_nan_test(self, preds, labels):\n",
    "        mse_loss = torch.nn.MSELoss(reduction='mean')\n",
    "        per_task_loss = torch.zeros(labels.size(1), device=device)\n",
    "        per_sample_loss = torch.zeros(labels.size(0), device=device)\n",
    "\n",
    "        \n",
    "        for k in range(labels.size(1)):\n",
    "            per_task_loss[k] = mse_loss(preds[~torch.isnan(labels[:,k]), k], labels[~torch.isnan(labels[:,k]), k])\n",
    "        \n",
    "        #per class loss\n",
    "        for k in range(labels.size(0)):\n",
    "            per_sample_loss[k] = mse_loss(preds[k, ~torch.isnan(labels[k,:])], labels[k, ~torch.isnan(labels[k, :])])\n",
    "        \n",
    "        \n",
    "        return torch.mean(per_task_loss[~torch.isnan(per_task_loss)]), per_task_loss,per_sample_loss\n",
    "    \n",
    "    def binary_classification_loss(self, preds, labels):\n",
    "        bin_loss = torch.nn.BCELoss()\n",
    "        return bin_loss(preds, labels)\n",
    "    \n",
    "    def train(self):\n",
    "\n",
    "        optimizer = torch.optim.Adagrad([\n",
    "            {'params': self.feature_extractor.parameters()},\n",
    "            {'params': self.latent_basis.parameters()},\n",
    "            {'params': self.MTL.parameters()},\n",
    "            {'params': self.decTTF.parameters()},\n",
    "            {'params': self.task_classifier.parameters()},\n",
    "            {'params': self.log_vars}\n",
    "        ], lr=self.learning_rate)\n",
    "        train_dataloader=self.train_dataloader()\n",
    "        for epoch in range(self.n_epoch):\n",
    "            for batch_idx,data in enumerate(train_dataloader):\n",
    "                x,y=data[0] #in vitro\n",
    "                x_unl=data[1][0] #invivo\n",
    "\n",
    "                p=float(batch_idx+epoch*self.len_dataloader)/self.n_epoch/self.len_dataloader\n",
    "                alpha=2. / (1. + np.exp(-10 * p)) - 1\n",
    "\n",
    "                preds_simulation=torch.zeros(y.size(0),y.size(1),self.passes)#,device=device) \n",
    "                for simulation in range(self.passes):\n",
    "                    preds, h, h_hat, domain_out_source = self.forward(x, alpha)\n",
    "                    preds_simulation[:,:, simulation]=preds\n",
    "                \n",
    "                preds_mean = torch.mean(preds_simulation, axis=2) #2D predictions after p passes\n",
    "                preds_var = torch.var(preds_simulation, axis=2) #2D\n",
    "                total_unc = torch.mean(preds_var, axis=0) # mu_t this is epistemic uncertainity 1D of tasks i.e y.shape[1]\n",
    "\n",
    "\n",
    "                m_loss, task_loss = self.task_mse_ignore_nan(preds_mean, y)\n",
    "                recon_loss= self.MSE_loss(h,h_hat)\n",
    "\n",
    "                a = 1 + (total_unc + torch.sum(torch.abs(self.decTTF[0].weight.T),1)) #multiplier to L_bnn\n",
    "\n",
    "                loss_weight = ( a[~torch.isnan(task_loss)] ) * task_loss[~torch.isnan(task_loss)]\n",
    "                loss_weight = torch.sum(loss_weight)\n",
    "\n",
    "                #Regularizer gamma(Z-sig(ZSA))\n",
    "                l1_S = self.mu * self.MTL.weight.norm(1)\n",
    "                L = self.latent_basis[0].weight.norm(2) + self.feature_extractor[0].weight.norm(2)\n",
    "                l2_L = self.lambda_ * L\n",
    "\n",
    "                #domain discriminator source\n",
    "                #normal data pass\n",
    "                # 0 is one class of invitro\n",
    "                zeros = torch.zeros(y.size(0),1)#, device=self.device)\n",
    "                d_loss_source = self.binary_classification_loss(domain_out_source, zeros)\n",
    "                \n",
    "                #domain discriminator target\n",
    "                #unlabelled data pass\n",
    "                #1 is second class which is invivo \n",
    "                preds, h, h_hat, domain_out_target = self.forward(x_unl, alpha)\n",
    "                ones = torch.ones(x_unl.size(0),1)#, device=self.device) \n",
    "                d_loss_target = self.binary_classification_loss(domain_out_target, ones)\n",
    "                \n",
    "                d_loss = d_loss_source + d_loss_target #l_adv loss or the adversarial loss \n",
    "\n",
    "                #total loss\n",
    "                loss = loss_weight + recon_loss + l1_S + l2_L + (self.lambda_disc *d_loss)\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                print({'total_loss': loss, 'source_loss': task_loss, 'disc_loss': d_loss})\n",
    "\n",
    "        return None #loss, m_loss, d_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7a54efc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Tugda_da(net_params, X_train, X_test, y_train, y_test)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4e54e7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tugda_da(nn.Module):\n",
    "\n",
    "    def __init__(self,params,train_data,test_data,y_train,y_test):\n",
    "        super(Tugda_da,self).__init__()\n",
    "        self.learning_rate = params['lr']\n",
    "        self.batch_size = params['bs_source']\n",
    "        self.mu = params['mu']\n",
    "        self.lambda_ = params['lambda_']\n",
    "        self.gamma = params['gamma']\n",
    "        self.lambda_disc = params['lambda_disc']\n",
    "        self.bs_disc = params['bs_disc']\n",
    "        self.n_epoch = params['n_epochs']\n",
    "        self.passes = params['passes']\n",
    "        self.num_tasks = params['num_tasks']\n",
    "\n",
    "        self.train_data = train_data\n",
    "        self.y_train = y_train\n",
    "        self.test_data = test_data\n",
    "        self.y_test = y_test\n",
    "\n",
    "        input_dim=self.train_data.shape[1]\n",
    "\n",
    "        feature_extractor = [nn.Linear(input_dim, params['hidden_units_1']), \n",
    "                             nn.Dropout(p=params['dropout']),\n",
    "                             nn.ReLU()]\n",
    "        \n",
    "        self.feature_extractor = nn.Sequential(*feature_extractor) #L layer in diag\n",
    "\n",
    "        latent_basis =  [nn.Linear(params['hidden_units_1'], params['latent_space']),\n",
    "                         nn.Dropout(p=params['dropout']),\n",
    "                         nn.ReLU()]\n",
    "        \n",
    "        self.latent_basis = nn.Sequential(*latent_basis) #Z layer in diag\n",
    "        \n",
    "        #task-specific weights\n",
    "        self.MTL = nn.Linear(params['latent_space'], self.num_tasks) #S Layer in diag| also the prediction outputs\n",
    "        \n",
    "        #decoder weights\n",
    "        decTTF = [nn.Linear( self.num_tasks , params['latent_space']), nn.ReLU()] # A layer in diag or decoder task to feature layer \n",
    "        self.decTTF = nn.Sequential(*decTTF) \n",
    "\n",
    "        #uncertainty (aleatoric)\n",
    "        self.log_vars = torch.zeros(self.num_tasks, requires_grad=True, device=device)\n",
    "        \n",
    "        #discriminator\n",
    "        task_classifier = [nn.Linear(params[\"latent_space\"], params[\"n_units_disc\"]), \n",
    "                             nn.Dropout(p=params['dropout']), \n",
    "                             nn.ReLU(),\n",
    "                             nn.Linear(params[\"n_units_disc\"], 1), \n",
    "                             nn.Sigmoid()\n",
    "                            ]\n",
    "        self.task_classifier = nn.Sequential(*task_classifier)\n",
    "\n",
    "        self.prepare_data()\n",
    "\n",
    "    def forward(self, input_data, alpha):\n",
    "        x = self.feature_extractor(input_data) #usually the input data is treated as x\n",
    "        x = self.latent_basis(x) # here h \n",
    "        preds = self.MTL(x)\n",
    "        x_hat = self.decTTF(preds) # task transfer to features # here was h_hat\n",
    "        reverse_feature = ReverseLayerF.apply(x, alpha)\n",
    "        task_classifier_output = self.task_classifier(reverse_feature)\n",
    "        return preds, x, x_hat, task_classifier_output\n",
    "    \n",
    "    def prepare_data(self):\n",
    "        train_dataset = TensorDataset(torch.FloatTensor(self.train_data),\n",
    "                                      torch.FloatTensor(self.y_train))\n",
    "\n",
    "        \n",
    "        target_unl_dataset = TensorDataset(torch.FloatTensor(self.test_data))\n",
    "        \n",
    "        self.train_dataset = train_dataset\n",
    "        self.target_unl_dataset = target_unl_dataset\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        dataloader1 = DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True) #in vitro data\n",
    "        dataloader2 = DataLoader(self.target_unl_dataset, batch_size=self.bs_disc, shuffle=True) #in vivo data \n",
    "        self.len_dataloader = min(len(dataloader1), len(dataloader2))\n",
    "        x=zip(dataloader1, cycle(dataloader2))\n",
    "        return x\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        dataloader = DataLoader(self.target_unl_dataset, batch_size=len(self.target_unl_dataset), shuffle=False)\n",
    "        return dataloader\n",
    "    \n",
    "    def MSE_loss(self, x, x_hat):\n",
    "        mse_loss = torch.nn.MSELoss()\n",
    "        return mse_loss(x, x_hat)\n",
    "    \n",
    "    def task_mse_ignore_nan(self, preds, labels):\n",
    "        mse_loss = torch.nn.MSELoss(reduction='none')\n",
    "        per_task_loss = torch.zeros(labels.size(1), device=device)\n",
    "\n",
    "        for k in range(labels.size(1)):\n",
    "            precision = torch.exp(-self.log_vars[k]) #didnt understand this.\n",
    "            diff = mse_loss(preds[~torch.isnan(labels[:,k]), k], labels[~torch.isnan(labels[:,k]), k])\n",
    "            per_task_loss[k] = torch.mean(precision * diff + self.log_vars[k])\n",
    "\n",
    "        return torch.mean(per_task_loss[~torch.isnan(per_task_loss)]), per_task_loss\n",
    "\n",
    "    def mse_ignore_nan_test(self, preds, labels):\n",
    "        mse_loss = torch.nn.MSELoss(reduction='mean')\n",
    "        per_task_loss = torch.zeros(labels.size(1), device=device)\n",
    "        per_sample_loss = torch.zeros(labels.size(0), device=device)\n",
    "\n",
    "        \n",
    "        for k in range(labels.size(1)):\n",
    "            per_task_loss[k] = mse_loss(preds[~torch.isnan(labels[:,k]), k], labels[~torch.isnan(labels[:,k]), k])\n",
    "        \n",
    "        #per class loss\n",
    "        for k in range(labels.size(0)):\n",
    "            per_sample_loss[k] = mse_loss(preds[k, ~torch.isnan(labels[k,:])], labels[k, ~torch.isnan(labels[k, :])])\n",
    "        \n",
    "        \n",
    "        return torch.mean(per_task_loss[~torch.isnan(per_task_loss)]), per_task_loss,per_sample_loss\n",
    "    \n",
    "    def binary_classification_loss(self, preds, labels):\n",
    "        bin_loss = torch.nn.BCELoss()\n",
    "        return bin_loss(preds, labels)\n",
    "    \n",
    "    def train(self):\n",
    "\n",
    "        optimizer = torch.optim.Adagrad([\n",
    "            {'params': self.feature_extractor.parameters()},\n",
    "            {'params': self.latent_basis.parameters()},\n",
    "            {'params': self.MTL.parameters()},\n",
    "            {'params': self.decTTF.parameters()},\n",
    "            {'params': self.task_classifier.parameters()},\n",
    "            {'params': self.log_vars}\n",
    "        ], lr=self.learning_rate)\n",
    "        train_dataloader=self.train_dataloader()\n",
    "        for epoch in range(self.n_epoch):\n",
    "            for batch_idx,data in enumerate(train_dataloader):\n",
    "                x,y=data[0] #in vitro\n",
    "                x_unl=data[1][0] #invivo\n",
    "\n",
    "                p=float(batch_idx+epoch*self.len_dataloader)/self.n_epoch/self.len_dataloader\n",
    "                alpha=2. / (1. + np.exp(-10 * p)) - 1\n",
    "\n",
    "                preds_simulation=torch.zeros(y.size(0),y.size(1),self.passes)#,device=device) \n",
    "                for simulation in range(self.passes):\n",
    "                    preds, h, h_hat, domain_out_source = self.forward(x, alpha)\n",
    "                    preds_simulation[:,:, simulation]=preds\n",
    "                \n",
    "                preds_mean = torch.mean(preds_simulation, axis=2) #2D predictions after p passes\n",
    "                preds_var = torch.var(preds_simulation, axis=2) #2D\n",
    "                total_unc = torch.mean(preds_var, axis=0) # mu_t this is epistemic uncertainity 1D of tasks i.e y.shape[1]\n",
    "\n",
    "\n",
    "                m_loss, task_loss = self.task_mse_ignore_nan(preds_mean, y)\n",
    "                recon_loss= self.MSE_loss(h,h_hat)\n",
    "\n",
    "                a = 1 + (total_unc + torch.sum(torch.abs(self.decTTF[0].weight.T),1)) #multiplier to L_bnn\n",
    "\n",
    "                loss_weight = ( a[~torch.isnan(task_loss)] ) * task_loss[~torch.isnan(task_loss)]\n",
    "                loss_weight = torch.sum(loss_weight)\n",
    "\n",
    "                #Regularizer gamma(Z-sig(ZSA))\n",
    "                l1_S = self.mu * self.MTL.weight.norm(1)\n",
    "                L = self.latent_basis[0].weight.norm(2) + self.feature_extractor[0].weight.norm(2)\n",
    "                l2_L = self.lambda_ * L\n",
    "\n",
    "                #domain discriminator source\n",
    "                #normal data pass\n",
    "                # 0 is one class of invitro\n",
    "                zeros = torch.zeros(y.size(0),1)#, device=self.device)\n",
    "                d_loss_source = self.binary_classification_loss(domain_out_source, zeros)\n",
    "                \n",
    "                #domain discriminator target\n",
    "                #unlabelled data pass\n",
    "                #1 is second class which is invivo \n",
    "                preds, h, h_hat, domain_out_target = self.forward(x_unl, alpha)\n",
    "                ones = torch.ones(x_unl.size(0),1)#, device=self.device) \n",
    "                d_loss_target = self.binary_classification_loss(domain_out_target, ones)\n",
    "                \n",
    "                d_loss = d_loss_source + d_loss_target #l_adv loss or the adversarial loss \n",
    "\n",
    "                #total loss\n",
    "                loss = loss_weight + recon_loss + l1_S + l2_L + (self.lambda_disc *d_loss)\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                print({'total_loss': loss})#, 'source_loss': task_loss, 'disc_loss': d_loss})\n",
    "\n",
    "        return None #loss, m_loss, d_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3ff90d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cell-line dataset;\n",
    "gdsc_dataset = pd.read_csv('data/GDSCDA_fpkm_AUC_all_drugs.zip', index_col=0)\n",
    "#gene set range\n",
    "gene_list = gdsc_dataset.columns[0:1780]\n",
    "drug_list = gdsc_dataset.columns[1780:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "fdc42a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pdx novartis dataset;\n",
    "pdx_dataset = pd.read_csv('data/PDX_MTL_DA.csv', index_col=0)\n",
    "drugs_pdx = pdx_dataset.columns[1780:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d71bb43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_params = {\n",
    " 'hidden_units_1': 1500,\n",
    " 'latent_space': 800,\n",
    " 'lr': 0.001,\n",
    " 'dropout': 0.1,\n",
    " 'mu': 1,\n",
    " 'lambda_': 1,\n",
    " 'gamma': 0.01,\n",
    " 'n_units_disc': 500,\n",
    " 'n_epochs': 5, #50\n",
    " 'bs_disc': 64,\n",
    " 'bs_source': 300,\n",
    " 'lambda_disc': 0.3,\n",
    " 'num_tasks': 200,\n",
    " 'passes': 10}#50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "beccd48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_list = gdsc_dataset.columns[0:1780]\n",
    "drug_list = gdsc_dataset.columns[1780:]\n",
    "\n",
    "X_train = gdsc_dataset[gene_list].values\n",
    "y_train = gdsc_dataset[drug_list].values\n",
    "\n",
    "X_test = pdx_dataset[gene_list].values\n",
    "y_test = pdx_dataset[drugs_pdx].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6ce3ccab",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Tugda_da(net_params, X_train, X_test, y_train, y_test)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "cf9cf1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_params = {\n",
    " 'hidden_units_1': 1500,\n",
    " 'latent_space': 800,\n",
    " 'lr': 0.001,\n",
    " 'dropout': 0.1,\n",
    " 'mu': 1,\n",
    " 'lambda_': 1,\n",
    " 'gamma': 0.01,\n",
    " 'n_units_disc': 500,\n",
    " 'n_epochs': 50, #50\n",
    " 'bs_disc': 64,\n",
    " 'bs_source': 300,\n",
    " 'lambda_disc': 0.3,\n",
    " 'num_tasks': 200,\n",
    " 'passes': 10}#50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "04abeb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_list = gdsc_dataset.columns[0:1780]\n",
    "drug_list = gdsc_dataset.columns[1780:]\n",
    "\n",
    "X_train = gdsc_dataset[gene_list].values\n",
    "y_train = gdsc_dataset[drug_list].values\n",
    "\n",
    "X_test = pdx_dataset[gene_list].values\n",
    "y_test = pdx_dataset[drugs_pdx].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "0d0238ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Tugda_da(net_params, X_train, X_test, y_train, y_test)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c1ef404e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tugda_da(nn.Module):\n",
    "\n",
    "    def __init__(self,params,train_data,test_data,y_train,y_test):\n",
    "        super(Tugda_da,self).__init__()\n",
    "        self.learning_rate = params['lr']\n",
    "        self.batch_size = params['bs_source']\n",
    "        self.mu = params['mu']\n",
    "        self.lambda_ = params['lambda_']\n",
    "        self.gamma = params['gamma']\n",
    "        self.lambda_disc = params['lambda_disc']\n",
    "        self.bs_disc = params['bs_disc']\n",
    "        self.n_epoch = params['n_epochs']\n",
    "        self.passes = params['passes']\n",
    "        self.num_tasks = params['num_tasks']\n",
    "\n",
    "        self.train_data = train_data\n",
    "        self.y_train = y_train\n",
    "        self.test_data = test_data\n",
    "        self.y_test = y_test\n",
    "\n",
    "        input_dim=self.train_data.shape[1]\n",
    "\n",
    "        feature_extractor = [nn.Linear(input_dim, params['hidden_units_1']), \n",
    "                             nn.Dropout(p=params['dropout']),\n",
    "                             nn.ReLU()]\n",
    "        \n",
    "        self.feature_extractor = nn.Sequential(*feature_extractor) #L layer in diag\n",
    "\n",
    "        latent_basis =  [nn.Linear(params['hidden_units_1'], params['latent_space']),\n",
    "                         nn.Dropout(p=params['dropout']),\n",
    "                         nn.ReLU()]\n",
    "        \n",
    "        self.latent_basis = nn.Sequential(*latent_basis) #Z layer in diag\n",
    "        \n",
    "        #task-specific weights\n",
    "        self.MTL = nn.Linear(params['latent_space'], self.num_tasks) #S Layer in diag| also the prediction outputs\n",
    "        \n",
    "        #decoder weights\n",
    "        decTTF = [nn.Linear( self.num_tasks , params['latent_space']), nn.ReLU()] # A layer in diag or decoder task to feature layer \n",
    "        self.decTTF = nn.Sequential(*decTTF) \n",
    "\n",
    "        #uncertainty (aleatoric)\n",
    "        self.log_vars = torch.zeros(self.num_tasks, requires_grad=True, device=device)\n",
    "        \n",
    "        #discriminator\n",
    "        task_classifier = [nn.Linear(params[\"latent_space\"], params[\"n_units_disc\"]), \n",
    "                             nn.Dropout(p=params['dropout']), \n",
    "                             nn.ReLU(),\n",
    "                             nn.Linear(params[\"n_units_disc\"], 1), \n",
    "                             nn.Sigmoid()\n",
    "                            ]\n",
    "        self.task_classifier = nn.Sequential(*task_classifier)\n",
    "\n",
    "        self.prepare_data()\n",
    "\n",
    "    def forward(self, input_data, alpha):\n",
    "        x = self.feature_extractor(input_data) #usually the input data is treated as x\n",
    "        x = self.latent_basis(x) # here h \n",
    "        preds = self.MTL(x)\n",
    "        x_hat = self.decTTF(preds) # task transfer to features # here was h_hat\n",
    "        reverse_feature = ReverseLayerF.apply(x, alpha)\n",
    "        task_classifier_output = self.task_classifier(reverse_feature)\n",
    "        return preds, x, x_hat, task_classifier_output\n",
    "    \n",
    "    def prepare_data(self):\n",
    "        train_dataset = TensorDataset(torch.FloatTensor(self.train_data),\n",
    "                                      torch.FloatTensor(self.y_train))\n",
    "\n",
    "        \n",
    "        target_unl_dataset = TensorDataset(torch.FloatTensor(self.test_data))\n",
    "        \n",
    "        self.train_dataset = train_dataset\n",
    "        self.target_unl_dataset = target_unl_dataset\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        dataloader1 = DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True) #in vitro data\n",
    "        dataloader2 = DataLoader(self.target_unl_dataset, batch_size=self.bs_disc, shuffle=True) #in vivo data \n",
    "        self.len_dataloader = min(len(dataloader1), len(dataloader2))\n",
    "        x=zip(dataloader1, cycle(dataloader2))\n",
    "        return x\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        dataloader = DataLoader(self.target_unl_dataset, batch_size=len(self.target_unl_dataset), shuffle=False)\n",
    "        return dataloader\n",
    "    \n",
    "    def MSE_loss(self, x, x_hat):\n",
    "        mse_loss = torch.nn.MSELoss()\n",
    "        return mse_loss(x, x_hat)\n",
    "    \n",
    "    def task_mse_ignore_nan(self, preds, labels):\n",
    "        mse_loss = torch.nn.MSELoss(reduction='none')\n",
    "        per_task_loss = torch.zeros(labels.size(1), device=device)\n",
    "\n",
    "        for k in range(labels.size(1)):\n",
    "            precision = torch.exp(-self.log_vars[k]) #didnt understand this.\n",
    "            diff = mse_loss(preds[~torch.isnan(labels[:,k]), k], labels[~torch.isnan(labels[:,k]), k])\n",
    "            per_task_loss[k] = torch.mean(precision * diff + self.log_vars[k])\n",
    "\n",
    "        return torch.mean(per_task_loss[~torch.isnan(per_task_loss)]), per_task_loss\n",
    "\n",
    "    def mse_ignore_nan_test(self, preds, labels):\n",
    "        mse_loss = torch.nn.MSELoss(reduction='mean')\n",
    "        per_task_loss = torch.zeros(labels.size(1), device=device)\n",
    "        per_sample_loss = torch.zeros(labels.size(0), device=device)\n",
    "\n",
    "        \n",
    "        for k in range(labels.size(1)):\n",
    "            per_task_loss[k] = mse_loss(preds[~torch.isnan(labels[:,k]), k], labels[~torch.isnan(labels[:,k]), k])\n",
    "        \n",
    "        #per class loss\n",
    "        for k in range(labels.size(0)):\n",
    "            per_sample_loss[k] = mse_loss(preds[k, ~torch.isnan(labels[k,:])], labels[k, ~torch.isnan(labels[k, :])])\n",
    "        \n",
    "        \n",
    "        return torch.mean(per_task_loss[~torch.isnan(per_task_loss)]), per_task_loss,per_sample_loss\n",
    "    \n",
    "    def binary_classification_loss(self, preds, labels):\n",
    "        bin_loss = torch.nn.BCELoss()\n",
    "        return bin_loss(preds, labels)\n",
    "    \n",
    "    def train(self):\n",
    "\n",
    "        optimizer = torch.optim.Adagrad([\n",
    "            {'params': self.feature_extractor.parameters()},\n",
    "            {'params': self.latent_basis.parameters()},\n",
    "            {'params': self.MTL.parameters()},\n",
    "            {'params': self.decTTF.parameters()},\n",
    "            {'params': self.task_classifier.parameters()},\n",
    "            {'params': self.log_vars}\n",
    "        ], lr=self.learning_rate)\n",
    "        train_dataloader=self.train_dataloader()\n",
    "        for epoch in range(self.n_epoch):\n",
    "            for batch_idx,data in enumerate(train_dataloader):\n",
    "                x,y=data[0] #in vitro\n",
    "                x_unl=data[1][0] #invivo\n",
    "\n",
    "                p=float(batch_idx+epoch*self.len_dataloader)/self.n_epoch/self.len_dataloader\n",
    "                alpha=2. / (1. + np.exp(-10 * p)) - 1\n",
    "\n",
    "                preds_simulation=torch.zeros(y.size(0),y.size(1),self.passes)#,device=device) \n",
    "                for simulation in range(self.passes):\n",
    "                    preds, h, h_hat, domain_out_source = self.forward(x, alpha)\n",
    "                    preds_simulation[:,:, simulation]=preds\n",
    "                \n",
    "                preds_mean = torch.mean(preds_simulation, axis=2) #2D predictions after p passes\n",
    "                preds_var = torch.var(preds_simulation, axis=2) #2D\n",
    "                total_unc = torch.mean(preds_var, axis=0) # mu_t this is epistemic uncertainity 1D of tasks i.e y.shape[1]\n",
    "\n",
    "\n",
    "                m_loss, task_loss = self.task_mse_ignore_nan(preds_mean, y)\n",
    "                recon_loss= self.MSE_loss(h,h_hat)\n",
    "\n",
    "                a = 1 + (total_unc + torch.sum(torch.abs(self.decTTF[0].weight.T),1)) #multiplier to L_bnn\n",
    "\n",
    "                loss_weight = ( a[~torch.isnan(task_loss)] ) * task_loss[~torch.isnan(task_loss)]\n",
    "                loss_weight = torch.sum(loss_weight)\n",
    "\n",
    "                #Regularizer gamma(Z-sig(ZSA))\n",
    "                l1_S = self.mu * self.MTL.weight.norm(1)\n",
    "                L = self.latent_basis[0].weight.norm(2) + self.feature_extractor[0].weight.norm(2)\n",
    "                l2_L = self.lambda_ * L\n",
    "\n",
    "                #domain discriminator source\n",
    "                #normal data pass\n",
    "                # 0 is one class of invitro\n",
    "                zeros = torch.zeros(y.size(0),1)#, device=self.device)\n",
    "                d_loss_source = self.binary_classification_loss(domain_out_source, zeros)\n",
    "                \n",
    "                #domain discriminator target\n",
    "                #unlabelled data pass\n",
    "                #1 is second class which is invivo \n",
    "                preds, h, h_hat, domain_out_target = self.forward(x_unl, alpha)\n",
    "                ones = torch.ones(x_unl.size(0),1)#, device=self.device) \n",
    "                d_loss_target = self.binary_classification_loss(domain_out_target, ones)\n",
    "                \n",
    "                d_loss = d_loss_source + d_loss_target #l_adv loss or the adversarial loss \n",
    "\n",
    "                #total loss\n",
    "                loss = loss_weight + recon_loss + l1_S + l2_L + (self.lambda_disc *d_loss)\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            print({'total_loss': loss})#, 'source_loss': task_loss, 'disc_loss': d_loss})\n",
    "        print(\"hello\")\n",
    "        return None #loss, m_loss, d_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7d8ede06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cell-line dataset;\n",
    "gdsc_dataset = pd.read_csv('data/GDSCDA_fpkm_AUC_all_drugs.zip', index_col=0)\n",
    "#gene set range\n",
    "gene_list = gdsc_dataset.columns[0:1780]\n",
    "drug_list = gdsc_dataset.columns[1780:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "9de7b3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pdx novartis dataset;\n",
    "pdx_dataset = pd.read_csv('data/PDX_MTL_DA.csv', index_col=0)\n",
    "drugs_pdx = pdx_dataset.columns[1780:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f602e7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_params = {\n",
    " 'hidden_units_1': 1500,\n",
    " 'latent_space': 800,\n",
    " 'lr': 0.001,\n",
    " 'dropout': 0.1,\n",
    " 'mu': 1,\n",
    " 'lambda_': 1,\n",
    " 'gamma': 0.01,\n",
    " 'n_units_disc': 500,\n",
    " 'n_epochs': 50, #50\n",
    " 'bs_disc': 64,\n",
    " 'bs_source': 300,\n",
    " 'lambda_disc': 0.3,\n",
    " 'num_tasks': 200,\n",
    " 'passes': 10}#50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "31dfc8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_list = gdsc_dataset.columns[0:1780]\n",
    "drug_list = gdsc_dataset.columns[1780:]\n",
    "\n",
    "X_train = gdsc_dataset[gene_list].values\n",
    "y_train = gdsc_dataset[drug_list].values\n",
    "\n",
    "X_test = pdx_dataset[gene_list].values\n",
    "y_test = pdx_dataset[drugs_pdx].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "49195995",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Tugda_da(net_params, X_train, X_test, y_train, y_test)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0d1c1fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tugda_da(nn.Module):\n",
    "\n",
    "    def __init__(self,params,train_data,test_data,y_train,y_test):\n",
    "        super(Tugda_da,self).__init__()\n",
    "        self.learning_rate = params['lr']\n",
    "        self.batch_size = params['bs_source']\n",
    "        self.mu = params['mu']\n",
    "        self.lambda_ = params['lambda_']\n",
    "        self.gamma = params['gamma']\n",
    "        self.lambda_disc = params['lambda_disc']\n",
    "        self.bs_disc = params['bs_disc']\n",
    "        self.n_epoch = params['n_epochs']\n",
    "        self.passes = params['passes']\n",
    "        self.num_tasks = params['num_tasks']\n",
    "\n",
    "        self.train_data = train_data\n",
    "        self.y_train = y_train\n",
    "        self.test_data = test_data\n",
    "        self.y_test = y_test\n",
    "\n",
    "        input_dim=self.train_data.shape[1]\n",
    "\n",
    "        feature_extractor = [nn.Linear(input_dim, params['hidden_units_1']), \n",
    "                             nn.Dropout(p=params['dropout']),\n",
    "                             nn.ReLU()]\n",
    "        \n",
    "        self.feature_extractor = nn.Sequential(*feature_extractor) #L layer in diag\n",
    "\n",
    "        latent_basis =  [nn.Linear(params['hidden_units_1'], params['latent_space']),\n",
    "                         nn.Dropout(p=params['dropout']),\n",
    "                         nn.ReLU()]\n",
    "        \n",
    "        self.latent_basis = nn.Sequential(*latent_basis) #Z layer in diag\n",
    "        \n",
    "        #task-specific weights\n",
    "        self.MTL = nn.Linear(params['latent_space'], self.num_tasks) #S Layer in diag| also the prediction outputs\n",
    "        \n",
    "        #decoder weights\n",
    "        decTTF = [nn.Linear( self.num_tasks , params['latent_space']), nn.ReLU()] # A layer in diag or decoder task to feature layer \n",
    "        self.decTTF = nn.Sequential(*decTTF) \n",
    "\n",
    "        #uncertainty (aleatoric)\n",
    "        self.log_vars = torch.zeros(self.num_tasks, requires_grad=True, device=device)\n",
    "        \n",
    "        #discriminator\n",
    "        task_classifier = [nn.Linear(params[\"latent_space\"], params[\"n_units_disc\"]), \n",
    "                             nn.Dropout(p=params['dropout']), \n",
    "                             nn.ReLU(),\n",
    "                             nn.Linear(params[\"n_units_disc\"], 1), \n",
    "                             nn.Sigmoid()\n",
    "                            ]\n",
    "        self.task_classifier = nn.Sequential(*task_classifier)\n",
    "\n",
    "        self.prepare_data()\n",
    "\n",
    "    def forward(self, input_data, alpha):\n",
    "        x = self.feature_extractor(input_data) #usually the input data is treated as x\n",
    "        x = self.latent_basis(x) # here h \n",
    "        preds = self.MTL(x)\n",
    "        x_hat = self.decTTF(preds) # task transfer to features # here was h_hat\n",
    "        reverse_feature = ReverseLayerF.apply(x, alpha)\n",
    "        task_classifier_output = self.task_classifier(reverse_feature)\n",
    "        return preds, x, x_hat, task_classifier_output\n",
    "    \n",
    "    def prepare_data(self):\n",
    "        train_dataset = TensorDataset(torch.FloatTensor(self.train_data),\n",
    "                                      torch.FloatTensor(self.y_train))\n",
    "\n",
    "        \n",
    "        target_unl_dataset = TensorDataset(torch.FloatTensor(self.test_data))\n",
    "        \n",
    "        self.train_dataset = train_dataset\n",
    "        self.target_unl_dataset = target_unl_dataset\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        dataloader1 = DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True) #in vitro data\n",
    "        dataloader2 = DataLoader(self.target_unl_dataset, batch_size=self.bs_disc, shuffle=True) #in vivo data \n",
    "        self.len_dataloader = min(len(dataloader1), len(dataloader2))\n",
    "        x=zip(dataloader1, cycle(dataloader2))\n",
    "        return x\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        dataloader = DataLoader(self.target_unl_dataset, batch_size=len(self.target_unl_dataset), shuffle=False)\n",
    "        return dataloader\n",
    "    \n",
    "    def MSE_loss(self, x, x_hat):\n",
    "        mse_loss = torch.nn.MSELoss()\n",
    "        return mse_loss(x, x_hat)\n",
    "    \n",
    "    def task_mse_ignore_nan(self, preds, labels):\n",
    "        mse_loss = torch.nn.MSELoss(reduction='none')\n",
    "        per_task_loss = torch.zeros(labels.size(1), device=device)\n",
    "\n",
    "        for k in range(labels.size(1)):\n",
    "            precision = torch.exp(-self.log_vars[k]) #didnt understand this.\n",
    "            diff = mse_loss(preds[~torch.isnan(labels[:,k]), k], labels[~torch.isnan(labels[:,k]), k])\n",
    "            per_task_loss[k] = torch.mean(precision * diff + self.log_vars[k])\n",
    "\n",
    "        return torch.mean(per_task_loss[~torch.isnan(per_task_loss)]), per_task_loss\n",
    "\n",
    "    def mse_ignore_nan_test(self, preds, labels):\n",
    "        mse_loss = torch.nn.MSELoss(reduction='mean')\n",
    "        per_task_loss = torch.zeros(labels.size(1), device=device)\n",
    "        per_sample_loss = torch.zeros(labels.size(0), device=device)\n",
    "\n",
    "        \n",
    "        for k in range(labels.size(1)):\n",
    "            per_task_loss[k] = mse_loss(preds[~torch.isnan(labels[:,k]), k], labels[~torch.isnan(labels[:,k]), k])\n",
    "        \n",
    "        #per class loss\n",
    "        for k in range(labels.size(0)):\n",
    "            per_sample_loss[k] = mse_loss(preds[k, ~torch.isnan(labels[k,:])], labels[k, ~torch.isnan(labels[k, :])])\n",
    "        \n",
    "        \n",
    "        return torch.mean(per_task_loss[~torch.isnan(per_task_loss)]), per_task_loss,per_sample_loss\n",
    "    \n",
    "    def binary_classification_loss(self, preds, labels):\n",
    "        bin_loss = torch.nn.BCELoss()\n",
    "        return bin_loss(preds, labels)\n",
    "    \n",
    "    def train(self):\n",
    "\n",
    "        optimizer = torch.optim.Adagrad([\n",
    "            {'params': self.feature_extractor.parameters()},\n",
    "            {'params': self.latent_basis.parameters()},\n",
    "            {'params': self.MTL.parameters()},\n",
    "            {'params': self.decTTF.parameters()},\n",
    "            {'params': self.task_classifier.parameters()},\n",
    "            {'params': self.log_vars}\n",
    "        ], lr=self.learning_rate)\n",
    "        \n",
    "        for epoch in range(self.n_epoch):\n",
    "            train_dataloader=self.train_dataloader()\n",
    "            for batch_idx,data in enumerate(train_dataloader):\n",
    "                x,y=data[0] #in vitro\n",
    "                x_unl=data[1][0] #invivo\n",
    "\n",
    "                p=float(batch_idx+epoch*self.len_dataloader)/self.n_epoch/self.len_dataloader\n",
    "                alpha=2. / (1. + np.exp(-10 * p)) - 1\n",
    "\n",
    "                preds_simulation=torch.zeros(y.size(0),y.size(1),self.passes)#,device=device) \n",
    "                for simulation in range(self.passes):\n",
    "                    preds, h, h_hat, domain_out_source = self.forward(x, alpha)\n",
    "                    preds_simulation[:,:, simulation]=preds\n",
    "                \n",
    "                preds_mean = torch.mean(preds_simulation, axis=2) #2D predictions after p passes\n",
    "                preds_var = torch.var(preds_simulation, axis=2) #2D\n",
    "                total_unc = torch.mean(preds_var, axis=0) # mu_t this is epistemic uncertainity 1D of tasks i.e y.shape[1]\n",
    "\n",
    "\n",
    "                m_loss, task_loss = self.task_mse_ignore_nan(preds_mean, y)\n",
    "                recon_loss= self.MSE_loss(h,h_hat)\n",
    "\n",
    "                a = 1 + (total_unc + torch.sum(torch.abs(self.decTTF[0].weight.T),1)) #multiplier to L_bnn\n",
    "\n",
    "                loss_weight = ( a[~torch.isnan(task_loss)] ) * task_loss[~torch.isnan(task_loss)]\n",
    "                loss_weight = torch.sum(loss_weight)\n",
    "\n",
    "                #Regularizer gamma(Z-sig(ZSA))\n",
    "                l1_S = self.mu * self.MTL.weight.norm(1)\n",
    "                L = self.latent_basis[0].weight.norm(2) + self.feature_extractor[0].weight.norm(2)\n",
    "                l2_L = self.lambda_ * L\n",
    "\n",
    "                #domain discriminator source\n",
    "                #normal data pass\n",
    "                # 0 is one class of invitro\n",
    "                zeros = torch.zeros(y.size(0),1)#, device=self.device)\n",
    "                d_loss_source = self.binary_classification_loss(domain_out_source, zeros)\n",
    "                \n",
    "                #domain discriminator target\n",
    "                #unlabelled data pass\n",
    "                #1 is second class which is invivo \n",
    "                preds, h, h_hat, domain_out_target = self.forward(x_unl, alpha)\n",
    "                ones = torch.ones(x_unl.size(0),1)#, device=self.device) \n",
    "                d_loss_target = self.binary_classification_loss(domain_out_target, ones)\n",
    "                \n",
    "                d_loss = d_loss_source + d_loss_target #l_adv loss or the adversarial loss \n",
    "\n",
    "                #total loss\n",
    "                loss = loss_weight + recon_loss + l1_S + l2_L + (self.lambda_disc *d_loss)\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            print({'total_loss': loss})#, 'source_loss': task_loss, 'disc_loss': d_loss})\n",
    "        return None #loss, m_loss, d_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "bccfd1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cell-line dataset;\n",
    "gdsc_dataset = pd.read_csv('data/GDSCDA_fpkm_AUC_all_drugs.zip', index_col=0)\n",
    "#gene set range\n",
    "gene_list = gdsc_dataset.columns[0:1780]\n",
    "drug_list = gdsc_dataset.columns[1780:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d5de474f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pdx novartis dataset;\n",
    "pdx_dataset = pd.read_csv('data/PDX_MTL_DA.csv', index_col=0)\n",
    "drugs_pdx = pdx_dataset.columns[1780:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "987f5575",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_params = {\n",
    " 'hidden_units_1': 1500,\n",
    " 'latent_space': 800,\n",
    " 'lr': 0.001,\n",
    " 'dropout': 0.1,\n",
    " 'mu': 1,\n",
    " 'lambda_': 1,\n",
    " 'gamma': 0.01,\n",
    " 'n_units_disc': 500,\n",
    " 'n_epochs': 10, #50\n",
    " 'bs_disc': 64,\n",
    " 'bs_source': 300,\n",
    " 'lambda_disc': 0.3,\n",
    " 'num_tasks': 200,\n",
    " 'passes': 10}#50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "1352458f",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_params = {\n",
    " 'hidden_units_1': 1500,\n",
    " 'latent_space': 800,\n",
    " 'lr': 0.001,\n",
    " 'dropout': 0.1,\n",
    " 'mu': 1,\n",
    " 'lambda_': 1,\n",
    " 'gamma': 0.01,\n",
    " 'n_units_disc': 500,\n",
    " 'n_epochs': 10, #50\n",
    " 'bs_disc': 64,\n",
    " 'bs_source': 300,\n",
    " 'lambda_disc': 0.3,\n",
    " 'num_tasks': 200,\n",
    " 'passes': 10}#50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "482eb826",
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_list = gdsc_dataset.columns[0:1780]\n",
    "drug_list = gdsc_dataset.columns[1780:]\n",
    "\n",
    "X_train = gdsc_dataset[gene_list].values\n",
    "y_train = gdsc_dataset[drug_list].values\n",
    "\n",
    "X_test = pdx_dataset[gene_list].values\n",
    "y_test = pdx_dataset[drugs_pdx].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ecef6ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Tugda_da(net_params, X_train, X_test, y_train, y_test)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "e3442a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_params = {\n",
    " 'hidden_units_1': 1500,\n",
    " 'latent_space': 800,\n",
    " 'lr': 0.001,\n",
    " 'dropout': 0.1,\n",
    " 'mu': 1,\n",
    " 'lambda_': 1,\n",
    " 'gamma': 0.01,\n",
    " 'n_units_disc': 500,\n",
    " 'n_epochs': 50, #50\n",
    " 'bs_disc': 64,\n",
    " 'bs_source': 300,\n",
    " 'lambda_disc': 0.3,\n",
    " 'num_tasks': 200,\n",
    " 'passes': 10}#50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "25e743b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Tugda_da(net_params, X_train, X_test, y_train, y_test)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "bbfa11cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_params = {\n",
    " 'hidden_units_1': 1500,\n",
    " 'latent_space': 800,\n",
    " 'lr': 0.001,\n",
    " 'dropout': 0.1,\n",
    " 'mu': 1,\n",
    " 'lambda_': 1,\n",
    " 'gamma': 0.01,\n",
    " 'n_units_disc': 500,\n",
    " 'n_epochs': 50, #50\n",
    " 'bs_disc': 64,\n",
    " 'bs_source': 300,\n",
    " 'lambda_disc': 0.3,\n",
    " 'num_tasks': 200,\n",
    " 'passes': 50}#50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "978d8555",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Tugda_da(net_params, X_train, X_test, y_train, y_test)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "ebc8887c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tugda_da(nn.Module):\n",
    "\n",
    "    def __init__(self,params,train_data,test_data,y_train,y_test):\n",
    "        super(Tugda_da,self).__init__()\n",
    "        self.learning_rate = params['lr']\n",
    "        self.batch_size = params['bs_source']\n",
    "        self.mu = params['mu']\n",
    "        self.lambda_ = params['lambda_']\n",
    "        self.gamma = params['gamma']\n",
    "        self.lambda_disc = params['lambda_disc']\n",
    "        self.bs_disc = params['bs_disc']\n",
    "        self.n_epoch = params['n_epochs']\n",
    "        self.passes = params['passes']\n",
    "        self.num_tasks = params['num_tasks']\n",
    "\n",
    "        self.train_data = train_data\n",
    "        self.y_train = y_train\n",
    "        self.test_data = test_data\n",
    "        self.y_test = y_test\n",
    "\n",
    "        input_dim=self.train_data.shape[1]\n",
    "\n",
    "        feature_extractor = [nn.Linear(input_dim, params['hidden_units_1']), \n",
    "                             nn.Dropout(p=params['dropout']),\n",
    "                             nn.ReLU()]\n",
    "        \n",
    "        self.feature_extractor = nn.Sequential(*feature_extractor) #L layer in diag\n",
    "\n",
    "        latent_basis =  [nn.Linear(params['hidden_units_1'], params['latent_space']),\n",
    "                         nn.Dropout(p=params['dropout']),\n",
    "                         nn.ReLU()]\n",
    "        \n",
    "        self.latent_basis = nn.Sequential(*latent_basis) #Z layer in diag\n",
    "        \n",
    "        #task-specific weights\n",
    "        self.MTL = nn.Linear(params['latent_space'], self.num_tasks) #S Layer in diag| also the prediction outputs\n",
    "        \n",
    "        #decoder weights\n",
    "        decTTF = [nn.Linear( self.num_tasks , params['latent_space']), nn.ReLU()] # A layer in diagram or decoder task to feature layer \n",
    "        self.decTTF = nn.Sequential(*decTTF) \n",
    "\n",
    "        #uncertainty (aleatoric)\n",
    "        self.log_vars = torch.zeros(self.num_tasks, requires_grad=True, device=device)\n",
    "        \n",
    "        #discriminator\n",
    "        task_classifier = [nn.Linear(params[\"latent_space\"], params[\"n_units_disc\"]), \n",
    "                             nn.Dropout(p=params['dropout']), \n",
    "                             nn.ReLU(),\n",
    "                             nn.Linear(params[\"n_units_disc\"], 1), \n",
    "                             nn.Sigmoid()\n",
    "                            ]\n",
    "        self.task_classifier = nn.Sequential(*task_classifier)\n",
    "\n",
    "        self.prepare_data()\n",
    "\n",
    "    def forward(self, input_data, alpha):\n",
    "        x = self.feature_extractor(input_data) #usually the input data is treated as x\n",
    "        x = self.latent_basis(x) # here h \n",
    "        preds = self.MTL(x)\n",
    "        x_hat = self.decTTF(preds) # task transfer to features # here was h_hat\n",
    "        reverse_feature = ReverseLayerF.apply(x, alpha)\n",
    "        task_classifier_output = self.task_classifier(reverse_feature)\n",
    "        return preds, x, x_hat, task_classifier_output\n",
    "    \n",
    "    def prepare_data(self):\n",
    "        train_dataset = TensorDataset(torch.FloatTensor(self.train_data),\n",
    "                                      torch.FloatTensor(self.y_train))\n",
    "\n",
    "        \n",
    "        target_unl_dataset = TensorDataset(torch.FloatTensor(self.test_data))\n",
    "        \n",
    "        self.train_dataset = train_dataset\n",
    "        self.target_unl_dataset = target_unl_dataset\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        dataloader1 = DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True) #in vitro data\n",
    "        dataloader2 = DataLoader(self.target_unl_dataset, batch_size=self.bs_disc, shuffle=True) #in vivo data \n",
    "        self.len_dataloader = min(len(dataloader1), len(dataloader2))\n",
    "        x=zip(dataloader1, cycle(dataloader2))\n",
    "        return x\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        dataloader = DataLoader(self.target_unl_dataset, batch_size=len(self.target_unl_dataset), shuffle=False)\n",
    "        return dataloader\n",
    "    \n",
    "    def MSE_loss(self, x, x_hat):\n",
    "        mse_loss = torch.nn.MSELoss()\n",
    "        return mse_loss(x, x_hat)\n",
    "    \n",
    "    def task_mse_ignore_nan(self, preds, labels):\n",
    "        mse_loss = torch.nn.MSELoss(reduction='none')\n",
    "        per_task_loss = torch.zeros(labels.size(1), device=device)\n",
    "\n",
    "        for k in range(labels.size(1)):\n",
    "            precision = torch.exp(-self.log_vars[k]) #didnt understand this.\n",
    "            diff = mse_loss(preds[~torch.isnan(labels[:,k]), k], labels[~torch.isnan(labels[:,k]), k])\n",
    "            per_task_loss[k] = torch.mean(precision * diff + self.log_vars[k])\n",
    "\n",
    "        return torch.mean(per_task_loss[~torch.isnan(per_task_loss)]), per_task_loss\n",
    "\n",
    "    def mse_ignore_nan_test(self, preds, labels):\n",
    "        mse_loss = torch.nn.MSELoss(reduction='mean')\n",
    "        per_task_loss = torch.zeros(labels.size(1), device=device)\n",
    "        per_sample_loss = torch.zeros(labels.size(0), device=device)\n",
    "\n",
    "        \n",
    "        for k in range(labels.size(1)):\n",
    "            per_task_loss[k] = mse_loss(preds[~torch.isnan(labels[:,k]), k], labels[~torch.isnan(labels[:,k]), k])\n",
    "        \n",
    "        #per class loss\n",
    "        for k in range(labels.size(0)):\n",
    "            per_sample_loss[k] = mse_loss(preds[k, ~torch.isnan(labels[k,:])], labels[k, ~torch.isnan(labels[k, :])])\n",
    "        \n",
    "        \n",
    "        return torch.mean(per_task_loss[~torch.isnan(per_task_loss)]), per_task_loss,per_sample_loss\n",
    "    \n",
    "    def binary_classification_loss(self, preds, labels):\n",
    "        bin_loss = torch.nn.BCELoss()\n",
    "        return bin_loss(preds, labels)\n",
    "    \n",
    "    def train(self):\n",
    "\n",
    "        optimizer = torch.optim.Adagrad([\n",
    "            {'params': self.feature_extractor.parameters()},\n",
    "            {'params': self.latent_basis.parameters()},\n",
    "            {'params': self.MTL.parameters()},\n",
    "            {'params': self.decTTF.parameters()},\n",
    "            {'params': self.task_classifier.parameters()},\n",
    "            {'params': self.log_vars}\n",
    "        ], lr=self.learning_rate)\n",
    "        \n",
    "        for epoch in range(self.n_epoch):\n",
    "            train_dataloader=self.train_dataloader()\n",
    "            train_loss=0\n",
    "            for batch_idx,data in enumerate(train_dataloader):\n",
    "                x,y=data[0] #in vitro\n",
    "                x_unl=data[1][0] #invivo\n",
    "\n",
    "                p=float(batch_idx+epoch*self.len_dataloader)/self.n_epoch/self.len_dataloader\n",
    "                alpha=2. / (1. + np.exp(-10 * p)) - 1\n",
    "\n",
    "                preds_simulation=torch.zeros(y.size(0),y.size(1),self.passes)#,device=device) \n",
    "                for simulation in range(self.passes):\n",
    "                    preds, h, h_hat, domain_out_source = self.forward(x, alpha)\n",
    "                    preds_simulation[:,:, simulation]=preds\n",
    "                \n",
    "                preds_mean = torch.mean(preds_simulation, axis=2) #2D predictions after p passes\n",
    "                preds_var = torch.var(preds_simulation, axis=2) #2D\n",
    "                total_unc = torch.mean(preds_var, axis=0) # mu_t this is epistemic uncertainity 1D of tasks i.e y.shape[1]\n",
    "\n",
    "\n",
    "                m_loss, task_loss = self.task_mse_ignore_nan(preds_mean, y)\n",
    "                recon_loss= self.MSE_loss(h,h_hat)\n",
    "\n",
    "                a = 1 + (total_unc + torch.sum(torch.abs(self.decTTF[0].weight.T),1)) #multiplier to L_bnn\n",
    "\n",
    "                loss_weight = ( a[~torch.isnan(task_loss)] ) * task_loss[~torch.isnan(task_loss)]\n",
    "                loss_weight = torch.sum(loss_weight)\n",
    "\n",
    "                #Regularizer gamma(Z-sig(ZSA))\n",
    "                l1_S = self.mu * self.MTL.weight.norm(1)\n",
    "                L = self.latent_basis[0].weight.norm(2) + self.feature_extractor[0].weight.norm(2)\n",
    "                l2_L = self.lambda_ * L\n",
    "\n",
    "                #domain discriminator source\n",
    "                #normal data pass\n",
    "                # 0 is one class of invitro\n",
    "                zeros = torch.zeros(y.size(0),1)#, device=self.device)\n",
    "                d_loss_source = self.binary_classification_loss(domain_out_source, zeros)\n",
    "                \n",
    "                #domain discriminator target\n",
    "                #unlabelled data pass\n",
    "                #1 is second class which is invivo \n",
    "                preds, h, h_hat, domain_out_target = self.forward(x_unl, alpha)\n",
    "                ones = torch.ones(x_unl.size(0),1)#, device=self.device) \n",
    "                d_loss_target = self.binary_classification_loss(domain_out_target, ones)\n",
    "                \n",
    "                d_loss = d_loss_source + d_loss_target #l_adv loss or the adversarial loss \n",
    "\n",
    "                #total loss\n",
    "                loss = loss_weight + recon_loss + l1_S + l2_L + (self.lambda_disc *d_loss)\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                train_loss+=loss\n",
    "\n",
    "            train_loss/=self.len_dataloader\n",
    "            print('Epoch [{}/{}] : Loss {}'.format(epoch,self.n_epoch,train_loss))#, 'source_loss': task_loss, 'disc_loss': d_loss})\n",
    "        return None #loss, m_loss, d_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "9dee6ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Tugda_da(net_params, X_train, X_test, y_train, y_test)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "26963513",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tugda_da(nn.Module):\n",
    "\n",
    "    def __init__(self,params,train_data,test_data,y_train,y_test):\n",
    "        super(Tugda_da,self).__init__()\n",
    "        self.learning_rate = params['lr']\n",
    "        self.batch_size = params['bs_source']\n",
    "        self.mu = params['mu']\n",
    "        self.lambda_ = params['lambda_']\n",
    "        self.gamma = params['gamma']\n",
    "        self.lambda_disc = params['lambda_disc']\n",
    "        self.bs_disc = params['bs_disc']\n",
    "        self.n_epoch = params['n_epochs']\n",
    "        self.passes = params['passes']\n",
    "        self.num_tasks = params['num_tasks']\n",
    "\n",
    "        self.train_data = train_data\n",
    "        self.y_train = y_train\n",
    "        self.test_data = test_data\n",
    "        self.y_test = y_test\n",
    "\n",
    "        input_dim=self.train_data.shape[1]\n",
    "\n",
    "        feature_extractor = [nn.Linear(input_dim, params['hidden_units_1']), \n",
    "                             nn.Dropout(p=params['dropout']),\n",
    "                             nn.ReLU()]\n",
    "        \n",
    "        self.feature_extractor = nn.Sequential(*feature_extractor) #L layer in diag\n",
    "\n",
    "        latent_basis =  [nn.Linear(params['hidden_units_1'], params['latent_space']),\n",
    "                         nn.Dropout(p=params['dropout']),\n",
    "                         nn.ReLU()]\n",
    "        \n",
    "        self.latent_basis = nn.Sequential(*latent_basis) #Z layer in diag\n",
    "        \n",
    "        #task-specific weights\n",
    "        self.MTL = nn.Linear(params['latent_space'], self.num_tasks) #S Layer in diag| also the prediction outputs\n",
    "        \n",
    "        #decoder weights\n",
    "        decTTF = [nn.Linear( self.num_tasks , params['latent_space']), nn.ReLU()] # A layer in diagram or decoder task to feature layer \n",
    "        self.decTTF = nn.Sequential(*decTTF) \n",
    "\n",
    "        #uncertainty (aleatoric)\n",
    "        self.log_vars = torch.zeros(self.num_tasks, requires_grad=True, device=device)\n",
    "        \n",
    "        #discriminator\n",
    "        task_classifier = [nn.Linear(params[\"latent_space\"], params[\"n_units_disc\"]), \n",
    "                             nn.Dropout(p=params['dropout']), \n",
    "                             nn.ReLU(),\n",
    "                             nn.Linear(params[\"n_units_disc\"], 1), \n",
    "                             nn.Sigmoid()\n",
    "                            ]\n",
    "        self.task_classifier = nn.Sequential(*task_classifier)\n",
    "\n",
    "        self.prepare_data()\n",
    "\n",
    "    def forward(self, input_data, alpha):\n",
    "        x = self.feature_extractor(input_data) #usually the input data is treated as x\n",
    "        x = self.latent_basis(x) # here h \n",
    "        preds = self.MTL(x)\n",
    "        x_hat = self.decTTF(preds) # task transfer to features # here was h_hat\n",
    "        reverse_feature = ReverseLayerF.apply(x, alpha)\n",
    "        task_classifier_output = self.task_classifier(reverse_feature)\n",
    "        return preds, x, x_hat, task_classifier_output\n",
    "    \n",
    "    def prepare_data(self):\n",
    "        train_dataset = TensorDataset(torch.FloatTensor(self.train_data),\n",
    "                                      torch.FloatTensor(self.y_train))\n",
    "\n",
    "        \n",
    "        target_unl_dataset = TensorDataset(torch.FloatTensor(self.test_data))\n",
    "        \n",
    "        self.train_dataset = train_dataset\n",
    "        self.target_unl_dataset = target_unl_dataset\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        dataloader1 = DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True) #in vitro data\n",
    "        dataloader2 = DataLoader(self.target_unl_dataset, batch_size=self.bs_disc, shuffle=True) #in vivo data \n",
    "        self.len_dataloader = min(len(dataloader1), len(dataloader2))\n",
    "        x=zip(dataloader1, cycle(dataloader2))\n",
    "        return x\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        dataloader = DataLoader(self.target_unl_dataset, batch_size=len(self.target_unl_dataset), shuffle=False)\n",
    "        return dataloader\n",
    "    \n",
    "    def MSE_loss(self, x, x_hat):\n",
    "        mse_loss = torch.nn.MSELoss()\n",
    "        return mse_loss(x, x_hat)\n",
    "    \n",
    "    def task_mse_ignore_nan(self, preds, labels):\n",
    "        mse_loss = torch.nn.MSELoss(reduction='none')\n",
    "        per_task_loss = torch.zeros(labels.size(1), device=device)\n",
    "\n",
    "        for k in range(labels.size(1)):\n",
    "            precision = torch.exp(-self.log_vars[k]) #didnt understand this.\n",
    "            diff = mse_loss(preds[~torch.isnan(labels[:,k]), k], labels[~torch.isnan(labels[:,k]), k])\n",
    "            per_task_loss[k] = torch.mean(precision * diff + self.log_vars[k])\n",
    "\n",
    "        return torch.mean(per_task_loss[~torch.isnan(per_task_loss)]), per_task_loss\n",
    "\n",
    "    def mse_ignore_nan_test(self, preds, labels):\n",
    "        mse_loss = torch.nn.MSELoss(reduction='mean')\n",
    "        per_task_loss = torch.zeros(labels.size(1), device=device)\n",
    "        per_sample_loss = torch.zeros(labels.size(0), device=device)\n",
    "\n",
    "        \n",
    "        for k in range(labels.size(1)):\n",
    "            per_task_loss[k] = mse_loss(preds[~torch.isnan(labels[:,k]), k], labels[~torch.isnan(labels[:,k]), k])\n",
    "        \n",
    "        #per class loss\n",
    "        for k in range(labels.size(0)):\n",
    "            per_sample_loss[k] = mse_loss(preds[k, ~torch.isnan(labels[k,:])], labels[k, ~torch.isnan(labels[k, :])])\n",
    "        \n",
    "        \n",
    "        return torch.mean(per_task_loss[~torch.isnan(per_task_loss)]), per_task_loss,per_sample_loss\n",
    "    \n",
    "    def binary_classification_loss(self, preds, labels):\n",
    "        bin_loss = torch.nn.BCELoss()\n",
    "        return bin_loss(preds, labels)\n",
    "    \n",
    "    def train(self):\n",
    "\n",
    "        optimizer = torch.optim.Adagrad([\n",
    "            {'params': self.feature_extractor.parameters()},\n",
    "            {'params': self.latent_basis.parameters()},\n",
    "            {'params': self.MTL.parameters()},\n",
    "            {'params': self.decTTF.parameters()},\n",
    "            {'params': self.task_classifier.parameters()},\n",
    "            {'params': self.log_vars}\n",
    "        ], lr=self.learning_rate)\n",
    "        \n",
    "        for epoch in range(self.n_epoch):\n",
    "            train_dataloader=self.train_dataloader()\n",
    "            train_loss=0\n",
    "            for batch_idx,data in enumerate(train_dataloader):\n",
    "                x,y=data[0] #in vitro\n",
    "                x_unl=data[1][0] #invivo\n",
    "\n",
    "                p=float(batch_idx+epoch*self.len_dataloader)/self.n_epoch/self.len_dataloader\n",
    "                alpha=2. / (1. + np.exp(-10 * p)) - 1\n",
    "\n",
    "                preds_simulation=torch.zeros(y.size(0),y.size(1),self.passes)#,device=device) \n",
    "                for simulation in range(self.passes):\n",
    "                    preds, h, h_hat, domain_out_source = self.forward(x, alpha)\n",
    "                    preds_simulation[:,:, simulation]=preds\n",
    "                \n",
    "                preds_mean = torch.mean(preds_simulation, axis=2) #2D predictions after p passes\n",
    "                preds_var = torch.var(preds_simulation, axis=2) #2D\n",
    "                total_unc = torch.mean(preds_var, axis=0) # mu_t this is epistemic uncertainity 1D of tasks i.e y.shape[1]\n",
    "\n",
    "\n",
    "                m_loss, task_loss = self.task_mse_ignore_nan(preds_mean, y)\n",
    "                recon_loss= self.MSE_loss(h,h_hat)\n",
    "\n",
    "                a = 1 + (total_unc + torch.sum(torch.abs(self.decTTF[0].weight.T),1)) #multiplier to L_bnn\n",
    "\n",
    "                loss_weight = ( a[~torch.isnan(task_loss)] ) * task_loss[~torch.isnan(task_loss)]\n",
    "                loss_weight = torch.sum(loss_weight)\n",
    "\n",
    "                #Regularizer gamma(Z-sig(ZSA))\n",
    "                l1_S = self.mu * self.MTL.weight.norm(1)\n",
    "                L = self.latent_basis[0].weight.norm(2) + self.feature_extractor[0].weight.norm(2)\n",
    "                l2_L = self.lambda_ * L\n",
    "\n",
    "                #domain discriminator source\n",
    "                #normal data pass\n",
    "                # 0 is one class of invitro\n",
    "                zeros = torch.zeros(y.size(0),1)#, device=self.device)\n",
    "                d_loss_source = self.binary_classification_loss(domain_out_source, zeros)\n",
    "                \n",
    "                #domain discriminator target\n",
    "                #unlabelled data pass\n",
    "                #1 is second class which is invivo \n",
    "                preds, h, h_hat, domain_out_target = self.forward(x_unl, alpha)\n",
    "                ones = torch.ones(x_unl.size(0),1)#, device=self.device) \n",
    "                d_loss_target = self.binary_classification_loss(domain_out_target, ones)\n",
    "                \n",
    "                d_loss = d_loss_source + d_loss_target #l_adv loss or the adversarial loss \n",
    "\n",
    "                #total loss\n",
    "                loss = loss_weight + recon_loss + l1_S + l2_L + (self.lambda_disc *d_loss)\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                train_loss+=loss\n",
    "\n",
    "            train_loss/=self.len_dataloader\n",
    "            print('Epoch [{}/{}] : Loss {}'.format(epoch,self.n_epoch,train_loss))#, 'source_loss': task_loss, 'disc_loss': d_loss})\n",
    "        return None #loss, m_loss, d_loss\n",
    "    \n",
    "    def test(self):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "50230afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_params = {\n",
    " 'hidden_units_1': 1500,\n",
    " 'latent_space': 800,\n",
    " 'lr': 0.001,\n",
    " 'dropout': 0.1,\n",
    " 'mu': 1,\n",
    " 'lambda_': 1,\n",
    " 'gamma': 0.01,\n",
    " 'n_units_disc': 500,\n",
    " 'n_epochs': 150, #50\n",
    " 'bs_disc': 64,\n",
    " 'bs_source': 300,\n",
    " 'lambda_disc': 0.3,\n",
    " 'num_tasks': 200,\n",
    " 'passes': 50}#50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "023a1654",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_params = {\n",
    " 'hidden_units_1': 1500,\n",
    " 'latent_space': 800,\n",
    " 'lr': 0.001,\n",
    " 'dropout': 0.1,\n",
    " 'mu': 1,\n",
    " 'lambda_': 1,\n",
    " 'gamma': 0.01,\n",
    " 'n_units_disc': 500,\n",
    " 'n_epochs': 150, #50\n",
    " 'bs_disc': 64,\n",
    " 'bs_source': 300,\n",
    " 'lambda_disc': 0.3,\n",
    " 'num_tasks': 200,\n",
    " 'passes': 20}#50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "326f35eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Tugda_da(net_params, X_train, X_test, y_train, y_test)\n",
    "model.train()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
